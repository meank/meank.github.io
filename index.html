<!doctype html>
<html class="theme-next   use-motion ">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.4.5.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.2" />






<meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
<meta name="twitter:description">



<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post',
    motion: true
  };
</script>

  <title> Hexo </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  






  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Hexo</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首頁
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            歸檔
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            標籤
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content">
          
  <section id="posts" class="posts-expand">

    

    
    

    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2015/12/08/‎Summarize/keggle-浮游生物分类--译文（一）/" itemprop="url">
                  kaggle-浮游生物分类比赛一等奖---译文（第一部分）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2015-12-08T13:08:04+08:00" content="2015-12-08">
              2015-12-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Summarize/" itemprop="url" rel="index">
                    <span itemprop="name">Summarize</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody">
            
              <p>原文 ：<a href="http://benanne.github.io/2015/03/17/plankton.html" target="_blank" rel="external">Classifying plankton with deep neural network</a></p>
<p>code：<a href="https://github.com/benanne/kaggle-ndsb" target="_blank" rel="external">code</a></p>
<p>作者在这次的比赛中获得了一等奖，team name是Deep Sea</p>
<h2 id="u6982_u8981"><a href="#u6982_u8981" class="headerlink" title="概要"></a>概要</h2><ul>
<li>1）介绍</li>
<li>2）预处理和data augmentation</li>
<li>3）network 结构</li>
<li>4）模型训练</li>
<li>5）非监督和半监督方法</li>
<li>6）model averaging</li>
<li>7）汇总（Miscellany）</li>
<li>8）总结</li>
</ul>
<h2 id="u4ECB_u7ECD"><a href="#u4ECB_u7ECD" class="headerlink" title="介绍"></a>介绍</h2><h3 id="1_uFF09_u8981_u89E3_u51B3_u7684_u95EE_u9898_uFF1A"><a href="#1_uFF09_u8981_u89E3_u51B3_u7684_u95EE_u9898_uFF1A" class="headerlink" title="1）要解决的问题："></a>1）要解决的问题：</h3><p>这个比赛是要对121类的海洋浮游生物的灰度图片进行分类，这些图片由水下摄像机拍摄。科学家用这些图片来判断哪些浮游生物物种会出现在这个区域。这类图片有非常多，在科学家做这些判断之前必须对这些图片进行标注，自动标注则能节省大量的时间。</p>
<p>比赛用的图片已经通过分割算法进行个体生物的识别与分离，并且进行了切割。最后的图片依各类浮游生物的大小不一，切割后的图片的大小也不一样，有趣的是，图片中的浮游生物得大小和实际的大小一致（与距摄像头的距离无关，可以理解为进行了认为处理）。也就是说，训练集的图片的size的大小是不一样的。</p>
<p>大赛希望参赛者能够建立一个模型，能够得到121类中每一张图片的概率分布，接着使用log loss进行打分，对应负似然方程（negative log likelihood）以及交叉熵(cross-entropy loss)。</p>
<p>对于这个比赛，loss 方程有几个有趣的特性：首先，如果你预测一张图片的置信度为1，但是却预测为错误的类，那么loss 方程的值会变得无限大。其次，它又是可区分的，也就是说，可以通过一个基于梯度的方法来求解（比如NN）；并且不需要使用surrogate loss 方程。</p>
<p>优化loss 方程并不完全等同于优化分类正确率。虽然这两个很明显是相关联的，但是我们观察到，经常出现loss 方程的优化有明显的提升，但是分类正确率却几乎没有什么影响。</p>
<h3 id="2_uFF09_u89E3_u51B3_u65B9_u6848_uFF1AConVNets_uFF01"><a href="#2_uFF09_u89E3_u51B3_u65B9_u6848_uFF1AConVNets_uFF01" class="headerlink" title="2）解决方案：ConVNets！"></a>2）解决方案：ConVNets！</h3><p>因为ConVNet在一些困难得task总是能够打破以前的记录，所以现在很多的图像分类问题采用的模型都是ConVNet。</p>
<p>对于这个竞赛来说，数据集过小是一个很大的挑战：共有30000张图片，分为121类，有的类别的图片甚至仅仅只有20张。深度学习普遍被认为需要非常多的数据才能获得好的结果，但是这个观念正在被打破，而我们得到的结果也表明这并不是非却不可。像dropout，weight decay, data augmentation, pre-training, pseudo-labeling和parameter sharing这些防止overfitting方法使得我们能够使用这个数据集训练一个包含27m参数的巨大网络。</p>
<p>作者也参加了<a href="http://benanne.github.io/2014/04/05/galaxy-zoo.html" target="_blank" rel="external">the Galaxy Challenge</a>比赛。比赛的目标是对星系图片进行分类，在the Galaxy Challenge使用的方法和技巧对于这个比赛同样适用。就像星系的图片一样，绝大部分的浮游生物得图片是 rotation invariant（表明可以对图片进行角度转换，flip等操作）的，利用这个特性可以进行有效的data augmentation。</p>
<h3 id="3_29_u4F7F_u7528_u7684_u8F6F_u4EF6_u4EE5_u53CA_u786C_u4EF6_u5E73_u53F0_uFF1A"><a href="#3_29_u4F7F_u7528_u7684_u8F6F_u4EF6_u4EE5_u53CA_u786C_u4EF6_u5E73_u53F0_uFF1A" class="headerlink" title="3)使用的软件以及硬件平台："></a>3)使用的软件以及硬件平台：</h3><p>我们使用了 Python, NumPy和Theano，也使用了cuDNN库，我们还使用了PyCUDA实现一些kernels。</p>
<p>我们使用了Lasagne，一个theano-based的深度学习包。</p>
<p>我们使用scikit-learn进行预处理和data augmentation，使用ghalton 生成随机数。竞赛中，我们的结果保存在Google Drive spreadsheet，代码托管在github的一个私有项目中。</p>
<p>使用的GPU：GTX 980, GTX 680 和 Tesla K40 cards.</p>
<h2 id="u9884_u5904_u7406_u548Cdata_augmentation"><a href="#u9884_u5904_u7406_u548Cdata_augmentation" class="headerlink" title="预处理和data augmentation"></a>预处理和data augmentation</h2><p>数据的预处理方面，我们只做了global zero mean unit variance (ZMUV)：首先计算每一个维度上数据的均值（使用全体数据计算），之后在每一个维度上都减去该均值。下一步便是在数据的每一维度上除以该维度上数据的标准差。此方法用于提升训练时的稳定性，并且加速收敛。</p>
<p>前面已经提到数据集的图片大小是大小不一的，最小的size是40 <em> 40，而最大的size是400 </em> 400。我们实验了很多种rescale的策略，大多数的情况下，我们将一张图片的最大边设置为固定大小。</p>
<p>我们也使用了图像矩（image moments），但是并没有提升结果。但是他们作为附加特性（additional features）对分类很有帮助，后面会讨论。</p>
<h3 id="1_uFF09data_augmentation"><a href="#1_uFF09data_augmentation" class="headerlink" title="1）data augmentation"></a>1）data augmentation</h3><p>我们人工的augment了数据集。我们使用了各种仿射变换(affine transforms)，逐渐的增加augmentation的程度，而我们的模型的overfit程度也随着增加。以下是我们data augmentation时的设置：</p>
<ul>
<li><strong>旋转（rotation）</strong>：0度至360度之间随机取值（uniform）</li>
<li><strong>translation</strong>：-10和10像素之间随机偏移（uniform）</li>
<li><strong>rescaling</strong>：比例为1/1.6和1.6之间的随机值（log-uniform）</li>
<li><strong>flipping</strong>: yes or no (bernoulli)</li>
<li><strong>剪切（shearing）</strong>：-20度至20度之间随机取值（uniform）</li>
<li><strong>延伸stretching</strong>：比例在1/1.3和1.3之间随机取值（log-uniform）</li>
</ul>
<p><img src="http://i.imgur.com/vq2qDH7.png" alt=""></p>
<p>我们进行的是realtime augmentation，也就是说GPU在进行训练时，CPU进行data augmentation的操作。</p>
<p>我们在一些点试验了弹性变形(elastic distortions),尽管有减小overfitting的作用，但是这并没有改善结果。在data augmentation的时候，我们将均匀（uniform）采样换成了高斯采样，这也没有改善结果。</p>
<h2 id="NetWork__u7ED3_u6784"><a href="#NetWork__u7ED3_u6784" class="headerlink" title="NetWork 结构"></a>NetWork 结构</h2><p>OxfordNet包含大量的Convolution layer（with 3 * 3 filter），我们使用与OxfordNet相同的Convolution结构：output feature map的大小 input feature map的大小相等。overlapping pooling使用window size = 3，stride = 2。</p>
<p>我们最先搭建了一个6层的浅层模型，然后逐渐的增加其层数，结果随着层数的增加变得越来越好。在比赛快结束的时候，我们使用的模型变成了一个16层的深度模型。面临的难题就是在提升的performance和增加的overfitting之间作权衡（译者个人理解：如果数据集涵盖了任务的所有可能性（feature），overfitting也没什么影响。如果是一个小的数据集，涵盖的任务的内容（feature）较少，可能更倾向于shallow模型来获取好的performance。应该和具体的任务和数据及有关，需要权衡。术语上：深度模型会造成overfitting，而浅层模型会降低overfitting，但是也会降低capasity）。</p>
<p>我们也参考了<a href="http://arxiv.org/abs/1502.01852" target="_blank" rel="external">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>,在第一，二层使用7*7的filter，但是并没有得到类似的performance。</p>
<h3 id="1_uFF09Cyclic_pooling"><a href="#1_uFF09Cyclic_pooling" class="headerlink" title="1）Cyclic pooling"></a>1）Cyclic pooling</h3><p>在参加 <a href="http://benanne.github.io/2014/04/05/galaxy-zoo.html" target="_blank" rel="external">Galaxy Challenge</a>的时候，我利用图像的旋转对称特性来共享网络的参数。这里我同样使用了与Galaxy Challenge中相同的Convolution layers作用于相同输入图片的不同版本（比如flip，rotated操作后），然后再将每个版本通过Convolution网络得到的feature representation连接成一个大向量，作为输入传递到dense layer（也就是FC层）。这使得网络使用同一个特征提取器来提取输入图片不同角度时的特征。</p>
<p>我们在此基础上又进一步：与其简单的将这些feature representation连接成大向量，不如在此基础上，对这个大向量进行pool操作以获得旋转不变性（rotation invariance）。实践中的具体操作：mini-batch中的每一张图片都会出现四次，分别对应于不同的旋转角度；再由并行的网络结果分别提取到feature representation，在最顶层，将这些feature representations连接起来后，进行pool操作。我们称之为cyclic pooling：</p>
<p><img src="http://i.imgur.com/k6ym7Bo.png" alt=""></p>
<p>4-way cyclic pooling 能非常高效的实现：旋转 0, 90, 180 和 270 度。</p>
<p>cyclic pooling也能够减少mini-batch的大小：比如原来data augmentation是offline，需要mini-batch的大小为128（可以理解为32张图片*4个角度），而使用cyclic pooling的realtime augmentation后，mini-batch的大小可以减小为32（32张原始图片）。（感觉这个没什么意义，需要处理的图片还是128张）。</p>
<p>我们试验了很多的 pool 方程（ before the output layer, between hidden layers），结果发现 root-mean-square pooling 取得了更好的结果。我们也不能解释为什么 root-mean-square pooling能取得更好的结果，我们猜测这根旋转相位不变性有关。</p>
<p>我们试验了两种8-way cyclic pooling：一种是将input图片每隔45度旋转一次，另一种则是将input图片进行flip后得到flip前后flip后的两张图，然后再分别将两张图每隔90度旋转一次，最后得到八张图片。遗憾的是这两种方法相对于4-way并没有获得更好的performance。</p>
<h3 id="2_29_u2018Rolling_u2019_feature_maps"><a href="#2_29_u2018Rolling_u2019_feature_maps" class="headerlink" title="2)‘Rolling’ feature maps"></a>2)‘Rolling’ feature maps</h3><p>cyclic pooling提高了我们的结果，但是在此基础上还能做一些工作。一个包含cyclic pooling的ConVNet可以提取一张输入图片四个方向（0,90,180,270）的feature representations，也可以解释为这个ConVNet的filters应用于同一输入图片的四个不同方向。也就是说我们可以将4组feature rmaps连接成大的feature maps，然后通过一些组合方式后，再输入下一层。最终的效果是：cyclic pooling ConVNet相当于包含四倍于其实际含有的filters的数量。</p>
<p>很容易对feature maps进行组合操作，因为feature maps已经计算好了。我们只需要将feature maps以正确的顺序和方位进行组合就可以。我们称这个操作为roll。</p>
<p><img src="http://i.imgur.com/Uq7WJDr.png" alt=""></p>
<p>Roll的操作可以在dense layer或者是Convolution layer后；当在Convolution layer后进行roll操作时，需要适当的旋转feature map（应该是考虑到输入图片不是正方形），以便他们能排成一条直线。</p>
<p>一开始我们基于theano完成了roll操作代码的编写。由于网络的训练随着roll layer的增加会变得越来越慢，后来我们编写了roll操作和其梯度的cuda kernel。使用PyCUDA将theano代码和自己编写的cuda kernel代码结合起来相对来说更简单一些，不需要编写额外的c代码。</p>
<p>我们更喜欢在Convolution + pool后进行roll操作，因为pool后的feature map相对更小，能够节约复制粘贴以及重新组合操作的时间。</p>
<p>需要注意的是，ConVNet可以有cyclic pooling层（without roll），但是不能只有roll而没有cyclic pooling层。因为roll的操作是在cyclic pooling层中对四个不同方向的feature map进行组合的操作（考虑到输入的是同一图片的四个不同方向）。</p>
<h3 id="3_uFF09Nonlinearities"><a href="#3_uFF09Nonlinearities" class="headerlink" title="3）Nonlinearities"></a>3）Nonlinearities</h3><p>我们试验了Relu及其各类各类变种，比如PRelu；在dense layer我们试验了maxout unit。我们还试验了smooth non-linearities。但是这些操作都很容易导致overfiting。</p>
<p>不过，我们发现LRelu对于我们的模型有很好的performance。</p>
<p>在相当一部分深度模型（10层以上）中，我们发现将PRelu的参数a设定在【0,1/2】对performance的影响不大，但是这个区域中的较大值（比如1/3）可以降低overfitting的程度。这让我们可以进一步按比例来扩充模型，最后我们选择的a为1/3。</p>
<h3 id="4_uFF09Spatial_pooling"><a href="#4_uFF09Spatial_pooling" class="headerlink" title="4）Spatial pooling"></a>4）Spatial pooling</h3><p>我们开始的时候使用了2-3个pool层，在最后阶段的大部分模型中，我们使用了4个pool层。</p>
<p>我们开始使用的是传统的 2 <em> 2 max-pooling，但是最终转换成stride为2的 3 </em> 3 max-pooling（记为3 * 3s2）。因为这能够使我们有更大的input size，并且能保持最顶上的Convolution层中的feature map的大小不变，而且计算消耗没有明显增加。</p>
<p>对于一个80 <em> 80的输入，网络有四个 2 </em> 2 的pooling层，假设得到的最终的feature map的大小是 5<em>5，那么如果我们用前面提到的 3 </em> 3s2来代替 传统的 2 <em> 2 pooling，那么我们可以使用97 </em> 97的input作为输入，但是最顶层的Convolution得到的feature map的大小依旧为 5 * 5。这样就能提升performance，而代价仅仅是稍微减慢训练速度。</p>
<h3 id="5_uFF09Multiscale_architectures"><a href="#5_uFF09Multiscale_architectures" class="headerlink" title="5）Multiscale architectures"></a>5）Multiscale architectures</h3><p>我们前面提到过数据集的图片的大小是不一样的，因此我们以图片的最大边为准来rescale图片。但是这明显不是最佳方案，因为图片的大小体现了浮游生物的真实大小，所以图片的大小包含了很多信息（feature）。</p>
<p>为了使模型能够学会这一点，我们试验了多种rescale策略，然后将不同种rescale得到的数据集输入同一个网络，组合成 ‘Multiscale’ 网络。</p>
<p>获得最好的performance的’Multiscale’ 网络是基于图片大小rescale的网络以及基于固定比例rescale小网络的组合。这确实是对训练速度有很大影响，但是也的确获得了些许的performance的提升。</p>
<h3 id="6_uFF09Additional_image_features"><a href="#6_uFF09Additional_image_features" class="headerlink" title="6）Additional image features"></a>6）Additional image features</h3><p>我们将附加特征输入到feature net小网络来纠正ConVNet的预测结果。我们把这叫做 ‘late fusing’，因为ConVNet与feature net（小网络）的连接一般位于ConVNet的最后几层（在softmax前）。我们也试验过在前面几层Convolution layer用feature net来纠正ConVNet，但是由于整个网络的训练参数过多，而造成了overfitting。</p>
<p>由于feature maps可以有原始图片输入ConVNet网络得到（例如，不同的input size得到的feature maps是不一样的），所以这可以作为附加特征进行分类。下面使我们试验过的一些获取附加特征的方法（加粗的是我们最终的模型使用了的）：</p>
<ul>
<li><strong>Image size in pixels</strong></li>
<li><strong>Size and shape estimates based on image moments</strong></li>
<li>Hu moments</li>
<li>Zernike moments</li>
<li>Parameter Free Threshold Adjacency Statistics</li>
<li>Linear Binary Patterns</li>
<li><strong>Haralick texture features</strong></li>
<li>Features from the competition tutorial</li>
<li>Combinations of the above</li>
</ul>
<p>image size, the features based on image moments 和 Haralick texture features 是获得最好的performance的几个附加特征。这些附加特征作为input，输入到一个两层的dense layer，共80个units。两层dense layer的最后一层与前面ConVNet生成的预测结果融合。因此，我们不需要重新训练ConVNet，也不需要重新生成ConVNet的预测结果，这节省了很多时间。</p>
<p>为了解决由于feature net权值随机初始化而造成的分歧，我们分别训练了十个网络（基于同样的输入，不同的权重随机初始化），得到十组权重，然后取十组权重的平均。这样做使得最终的valid loss降低了1.8%，在比赛的最后阶段，这样的效果是不容小觑的。</p>
<p>有趣的是，基于image size, the features based on image moments 和 Haralick texture features 的late fusing对最终结果就像 Multiscale net 之于 常规的ConVNet。有点违反直觉的是：image size, the features based on image moments 和 Haralick texture features这三种获取到的附加特征都是图像的size信息，本以为通过这三类特征的到的performance会有重叠（即用了image size再用 Haralick texture features时对performance的影响不大），结果却表明他们不怎么重叠。</p>
<h3 id="7_uFF09Example_convnet_architecture"><a href="#7_uFF09Example_convnet_architecture" class="headerlink" title="7）Example convnet architecture"></a>7）Example convnet architecture</h3><p>这是一个performance还不错的网络结构的例子。共13层（10个Convolution layers，3个FC layers）以及4个pooling层。输入shape：（32, 1, 95, 95），遵循bc01(batch size, number of channels, height, width)。输出shape：(32, 121)。对于给定的input，网络输出的是121  个probabilities，对应于121类，总的probabilities的值为1。</p>
<p><img src="http://i.imgur.com/svWgljA.png" alt=""></p>
<p> ‘cyclic slice’ layer将batch-size增加了四倍（128）， ‘cyclic pooling’ layer 在最后将batch-size重新减小了四倍（32）. ‘cyclic roll’ layers 将feature maps的数量增大了四倍。</p>
<p><strong>本篇译文系作者原创，转载请先联系作者: 18254275587@163.com</strong></p>

            
          </span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2015/12/08/UFLDL/DeepLearningbyAndrewNg---self-taught/" itemprop="url">
                  UFLDL-self-taught
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2015-12-08T13:08:04+08:00" content="2015-12-08">
              2015-12-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/UFLDL/" itemprop="url" rel="index">
                    <span itemprop="name">UFLDL</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody">
            
              <h2 id="u672C_u6B21UFLDL_u7EC3_u4E60_u5927_u81F4_u6D41_u7A0B_uFF1A"><a href="#u672C_u6B21UFLDL_u7EC3_u4E60_u5927_u81F4_u6D41_u7A0B_uFF1A" class="headerlink" title="本次UFLDL练习大致流程："></a>本次UFLDL练习大致流程：</h2><ul>
<li>通过对标记为5-9的数字图像进行self-taught特征提取（笔画特征），获得特征参数opttheta。</li>
<li>use opttheta to obtain a（2） which represente the labeled input data.</li>
<li>Training and testing the logistic regression model(with softmaxTrain.m which we have done previously).using the training set features (trainFeatures) and labels (trainLabels).</li>
<li>Classifying on the test set.completing the code to make predictions on the test set (testFeatures)</li>
</ul>
<h2 id="self-taught__u548Csemi-supervised_u7684_u533A_u522B_uFF1A"><a href="#self-taught__u548Csemi-supervised_u7684_u533A_u522B_uFF1A" class="headerlink" title="self-taught 和semi-supervised的区别："></a>self-taught 和semi-supervised的区别：</h2><ul>
<li>两者都是通过对大量未标记的数据进行特征提取（例如使用autoencoder，得到W），然后再将标记的数据输入，得到representation — 与输入相对应的a。然后再将得到的a作为classifier的输入进行分类（如softmax regression）。</li>
<li>不同：<br><strong>self-taught</strong>———Suppose your goal is a computer vision task where you’d like to distinguish between images of cars and images of motorcycles; so, each labeled example in your training set is either an image of a car or an image of a motorcycle. Where can we get lots of unlabeled data? The easiest way would be to obtain some random collection of images, perhaps downloaded off the internet. We could then train the autoencoder on this large collection of images, and obtain useful features from them. Because here the unlabeled data is drawn from a different distribution than the labeled data (i.e., perhaps some of our unlabeled images may contain cars/motorcycles, but not every image downloaded is either a car or a motorcycle), we call this self-taught learning.<br><strong>semi-supervised</strong>———In contrast, if we happen to have lots of unlabeled images lying around that are all images of either a car or a motorcycle, but where the data is just missing its label (so you don’t know which ones are cars, and which ones are motorcycles), then we could use this form of unlabeled data to learn the features. This setting—where each unlabeled example is drawn from the same distribution as your labeled examples—is sometimes called the semi-supervised setting.<h2 id="u7EC3_u4E60_u9898_u7B54_u6848_uFF08_u63A8_u8350_u81EA_u5DF1_u5B8C_u6210_u540E_u518D_u53C2_u8003_uFF09"><a href="#u7EC3_u4E60_u9898_u7B54_u6848_uFF08_u63A8_u8350_u81EA_u5DF1_u5B8C_u6210_u540E_u518D_u53C2_u8003_uFF09" class="headerlink" title="练习题答案（推荐自己完成后再参考）"></a>练习题答案（推荐自己完成后再参考）</h2></li>
<li>stlExercise.m</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% CS294A/CS294W Self-taught Learning Exercise</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%  Instructions</span></span><br><span class="line"><span class="comment">%  ------------</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">%  This file contains code that helps you get started on the</span></span><br><span class="line"><span class="comment">%  self-taught learning. You will need to complete code in feedForwardAutoencoder.m</span></span><br><span class="line"><span class="comment">%  You will also need to have implemented sparseAutoencoderCost.m and </span></span><br><span class="line"><span class="comment">%  softmaxCost.m from previous exercises.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%% ======================================================================</span></span><br><span class="line"><span class="comment">%  STEP 0: Here we provide the relevant parameters values that will</span></span><br><span class="line"><span class="comment">%  allow your sparse autoencoder to get good filters; you do not need to </span></span><br><span class="line"><span class="comment">%  change the parameters below.</span></span><br><span class="line"></span><br><span class="line">inputSize  = <span class="number">28</span> * <span class="number">28</span>;</span><br><span class="line">numLabels  = <span class="number">5</span>;</span><br><span class="line">hiddenSize = <span class="number">200</span>;</span><br><span class="line">sparsityParam = <span class="number">0.1</span>; <span class="comment">% desired average activation of the hidden units.</span></span><br><span class="line">                     <span class="comment">% (This was denoted by the Greek alphabet rho, which looks like a lower-case "p",</span></span><br><span class="line">		             <span class="comment">%  in the lecture notes). </span></span><br><span class="line">lambda = <span class="number">3e-3</span>;       <span class="comment">% weight decay parameter       </span></span><br><span class="line"><span class="built_in">beta</span> = <span class="number">3</span>;            <span class="comment">% weight of sparsity penalty term   </span></span><br><span class="line">maxIter = <span class="number">400</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ======================================================================</span></span><br><span class="line"><span class="comment">%  STEP 1: Load data from the MNIST database</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%  This loads our training and test data from the MNIST database files.</span></span><br><span class="line"><span class="comment">%  We have sorted the data for you in this so that you will not have to</span></span><br><span class="line"><span class="comment">%  change it.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Load MNIST database files</span></span><br><span class="line">mnistData   = loadMNISTImages(<span class="string">'train-images-idx3-ubyte'</span>);</span><br><span class="line">mnistLabels = loadMNISTLabels(<span class="string">'train-labels-idx1-ubyte'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Set Unlabeled Set (All Images)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Simulate a Labeled and Unlabeled set</span></span><br><span class="line">labeledSet   = <span class="built_in">find</span>(mnistLabels &gt;= <span class="number">0</span> &amp; mnistLabels &lt;= <span class="number">4</span>);</span><br><span class="line">unlabeledSet = <span class="built_in">find</span>(mnistLabels &gt;= <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">numTrain = <span class="built_in">round</span>(<span class="built_in">numel</span>(labeledSet)/<span class="number">2</span>);</span><br><span class="line">trainSet = labeledSet(<span class="number">1</span>:numTrain);</span><br><span class="line">testSet  = labeledSet(numTrain+<span class="number">1</span>:<span class="keyword">end</span>);</span><br><span class="line"></span><br><span class="line">unlabeledData = mnistData(:, unlabeledSet);</span><br><span class="line"></span><br><span class="line">trainData   = mnistData(:, trainSet);</span><br><span class="line">trainLabels = mnistLabels(trainSet)<span class="operator">'</span> + <span class="number">1</span>; <span class="comment">% Shift Labels to the Range 1-5</span></span><br><span class="line"></span><br><span class="line">testData   = mnistData(:, testSet);</span><br><span class="line">testLabels = mnistLabels(testSet)<span class="operator">'</span> + <span class="number">1</span>;   <span class="comment">% Shift Labels to the Range 1-5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Output Some Statistics</span></span><br><span class="line">fprintf(<span class="string">'# examples in unlabeled set: %d\n'</span>, <span class="built_in">size</span>(unlabeledData, <span class="number">2</span>));</span><br><span class="line">fprintf(<span class="string">'# examples in supervised training set: %d\n\n'</span>, <span class="built_in">size</span>(trainData, <span class="number">2</span>));</span><br><span class="line">fprintf(<span class="string">'# examples in supervised testing set: %d\n\n'</span>, <span class="built_in">size</span>(testData, <span class="number">2</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ======================================================================</span></span><br><span class="line"><span class="comment">%  STEP 2: Train the sparse autoencoder</span></span><br><span class="line"><span class="comment">%  This trains the sparse autoencoder on the unlabeled training</span></span><br><span class="line"><span class="comment">%  images. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">%  Randomly initialize the parameters</span></span><br><span class="line">theta = initializeParameters(hiddenSize, inputSize);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ----------------- YOUR CODE HERE ----------------------</span></span><br><span class="line"><span class="comment">%  Find opttheta by running the sparse autoencoder on</span></span><br><span class="line"><span class="comment">%  unlabeledTrainingImages</span></span><br><span class="line"></span><br><span class="line">opttheta = theta; </span><br><span class="line">addpath minFunc/</span><br><span class="line">options.Method = <span class="string">'lbfgs'</span>; <span class="comment">% Here, we use L-BFGS to optimize our cost</span></span><br><span class="line">                          <span class="comment">% function. Generally, for minFunc to work, you</span></span><br><span class="line">                          <span class="comment">% need a function pointer with two outputs: the</span></span><br><span class="line">                          <span class="comment">% function value and the gradient. In our problem,</span></span><br><span class="line">                          <span class="comment">% sparseAutoencoderCost.m satisfies this.</span></span><br><span class="line">options.maxIter = <span class="number">400</span>;	  <span class="comment">% Maximum number of iterations of L-BFGS to run </span></span><br><span class="line">options.display = <span class="string">'on'</span>;</span><br><span class="line"></span><br><span class="line"><span class="matrix">[opttheta, cost]</span> = minFunc( @(p) sparseAutoencoderCost(p, ...</span><br><span class="line">                                  inputSize, hiddenSize, ...</span><br><span class="line">                                   lambda, sparsityParam, ...</span><br><span class="line">                                   <span class="built_in">beta</span>, unlabeledData), ...</span><br><span class="line">                              theta, options);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% -----------------------------------------------------</span></span><br><span class="line">                          </span><br><span class="line"><span class="comment">% Visualize weights</span></span><br><span class="line">W1 = <span class="built_in">reshape</span>(opttheta(<span class="number">1</span>:hiddenSize * inputSize), hiddenSize, inputSize);</span><br><span class="line">display_network(W1<span class="operator">'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 3: Extract Features from the Supervised Dataset</span></span><br><span class="line"><span class="comment">%  </span></span><br><span class="line"><span class="comment">%  You need to complete the code in feedForwardAutoencoder.m so that the </span></span><br><span class="line"><span class="comment">%  following command will extract features from the data.</span></span><br><span class="line"></span><br><span class="line">trainFeatures = feedForwardAutoencoder(opttheta, hiddenSize, inputSize, ...</span><br><span class="line">                                       trainData);</span><br><span class="line"></span><br><span class="line">testFeatures = feedForwardAutoencoder(opttheta, hiddenSize, inputSize, ...</span><br><span class="line">                                       testData);</span><br><span class="line"></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 4: Train the softmax classifier</span></span><br><span class="line"></span><br><span class="line">softmaxModel = struct;  </span><br><span class="line"><span class="comment">%% ----------------- YOUR CODE HERE ----------------------</span></span><br><span class="line"><span class="comment">%  Use softmaxTrain.m from the previous exercise to train a multi-class</span></span><br><span class="line"><span class="comment">%  classifier. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">%  Use lambda = 1e-4 for the weight regularization for softmax</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% You need to compute softmaxModel using softmaxTrain on trainFeatures and</span></span><br><span class="line"><span class="comment">% trainLabels</span></span><br><span class="line">lambda = <span class="number">1e-4</span>;</span><br><span class="line">options.maxIter = <span class="number">100</span>;</span><br><span class="line">softmaxModel = softmaxTrain(hiddenSize, <span class="number">5</span>, lambda, ...</span><br><span class="line">                            trainFeatures,trainLabels, options);</span><br><span class="line">                        <span class="comment">%注意tainFeatures的大小的hiddenSize</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% -----------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 5: Testing </span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% ----------------- YOUR CODE HERE ----------------------</span></span><br><span class="line"><span class="comment">% Compute Predictions on the test set (testFeatures) using softmaxPredict</span></span><br><span class="line"><span class="comment">% and softmaxModel</span></span><br><span class="line"><span class="matrix">[pred]</span> = softmaxPredict(softmaxModel, testFeatures);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% -----------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Classification Score</span></span><br><span class="line">fprintf(<span class="string">'Test Accuracy: %f%%\n'</span>, <span class="number">100</span>*mean(pred(:) == testLabels(:)));</span><br><span class="line"></span><br><span class="line"><span class="comment">% (note that we shift the labels by 1, so that digit 0 now corresponds to</span></span><br><span class="line"><span class="comment">%  label 1)</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Accuracy is the proportion of correctly classified images</span></span><br><span class="line"><span class="comment">% The results for our implementation was:</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Accuracy: 98.3%</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%</span></span><br></pre></td></tr></table></figure>
<ul>
<li>feedForwardAutoencoder.m</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[activation]</span> = <span class="title">feedForwardAutoencoder</span><span class="params">(theta, hiddenSize, visibleSize, data)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">% theta: trained weights from the autoencoder</span></span><br><span class="line"><span class="comment">% visibleSize: the number of input units (probably 64) </span></span><br><span class="line"><span class="comment">% hiddenSize: the number of hidden units (probably 25) </span></span><br><span class="line"><span class="comment">% data: Our matrix containing the training data as columns.  So, data(:,i) is the i-th training example. </span></span><br><span class="line">  </span><br><span class="line"><span class="comment">% We first convert theta to the (W1, W2, b1, b2) matrix/vector format, so that this </span></span><br><span class="line"><span class="comment">% follows the notation convention of the lecture notes. </span></span><br><span class="line"></span><br><span class="line">W1 = <span class="built_in">reshape</span>(theta(<span class="number">1</span>:hiddenSize*visibleSize), hiddenSize, visibleSize);</span><br><span class="line">b1 = theta(<span class="number">2</span>*hiddenSize*visibleSize+<span class="number">1</span>:<span class="number">2</span>*hiddenSize*visibleSize+hiddenSize);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------- YOUR CODE HERE --------------------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Compute the activation of the hidden layer for the Sparse Autoencoder.</span></span><br><span class="line">m=<span class="built_in">size</span>(data,<span class="number">2</span>);</span><br><span class="line">z2 = W1*data+<span class="built_in">repmat</span>(b1,<span class="number">1</span>,m);<span class="comment">%注意这里一定要将b1向量复制扩展成m列的矩阵</span></span><br><span class="line">activation = sigmoid(z2);</span><br><span class="line"></span><br><span class="line"><span class="comment">%-------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%-------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">% Here's an implementation of the sigmoid function, which you may find useful</span></span><br><span class="line"><span class="comment">% in your computation of the costs and the gradients.  This inputs a (row or</span></span><br><span class="line"><span class="comment">% column) vector (say (z1, z2, z3)) and returns (f(z1), f(z2), f(z3)). </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sigm</span> = <span class="title">sigmoid</span><span class="params">(x)</span></span></span><br><span class="line">    sigm = <span class="number">1</span> ./ (<span class="number">1</span> + <span class="built_in">exp</span>(-x));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
            
          </span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2015/12/08/‎Summarize/keggle-浮游生物分类--译文（第二部分）/" itemprop="url">
                  kaggle-浮游生物分类比赛一等奖---译文（第二部分）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2015-12-08T13:08:03+08:00" content="2015-12-08">
              2015-12-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Summarize/" itemprop="url" rel="index">
                    <span itemprop="name">Summarize</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody">
            
              <p>接着<a href="http://meank.github.io/2015/10/07/%E2%80%8ESummarize/keggle-%E6%B5%AE%E6%B8%B8%E7%94%9F%E7%89%A9%E5%88%86%E7%B1%BB%E6%AF%94%E8%B5%9B%E4%B8%80%E7%AD%89%E5%A5%96---%E8%AF%91%E6%96%87/" target="_blank" rel="external">上一篇</a>的内容</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="1_uFF09validation"><a href="#1_uFF09validation" class="headerlink" title="1）validation"></a>1）validation</h3><p>我们使用分层抽样（stratified sampling）的方法，将已标注的数据集分出10%作为验证集（validation）。由于数据集过小，我们在验证集上的评估受到噪声的影响比较大，因此，我们也在排行榜上的其他模型上测试了我们的验证集。</p>
<h3 id="2_uFF09training_algorithm"><a href="#2_uFF09training_algorithm" class="headerlink" title="2）training algorithm"></a>2）training algorithm</h3><p>所有的模型都是在加了Nesterov momentum的SGD优化算法上进行的。我们将momentum的系数设置为0.9。大多数模型需要24-48小时收敛。</p>
<p>对于大多数模型，我们通过215000步梯度下降可以学习。最终的学习率采用类似于<a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank" rel="external"> Krizhevsky et al</a>的策略通过两次1/10的缩小进行调节，调节的步骤分别在第180000 和 205000步。我们一般使用0.003作为学习率的初始值。</p>
<p>同时我们也测试了<a href="http://arxiv.org/abs/1412.6980" target="_blank" rel="external">Adam</a>（使用的是第一个版本）作为Nesterov momentum的替代。结果：Adam能够加速两倍的收敛，但是最后的performance（相较于Nesterov momentum）会降低。所以我们放弃了Adam。</p>
<h3 id="3_uFF09initialization"><a href="#3_uFF09initialization" class="headerlink" title="3）initialization"></a>3）initialization</h3><p>我们使用了<a href="http://arxiv.org/abs/1312.6120" target="_blank" rel="external">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</a>中提出的正交初始化策略（orthogonal initialization strategy）的一个变种。这使得我们可以根据我们的需要去增加网络的深度，而不会在收敛时遇到问题。</p>
<h3 id="4_uFF09regularization"><a href="#4_uFF09regularization" class="headerlink" title="4）regularization"></a>4）regularization</h3><p>对于大多数模型，我们在全连接层使用dropout，dropout的概率设置为0.5。我们也在一些模型中在Convolution layers进行了dropout操作。</p>
<p>我们也尝试过Gaussian dropout（用multiplicative高斯噪声代替multiplicative伯努利噪声），但是发现performance和传统的dropout差不多。</p>
<p>在比赛快结束的时候，我们发现进行少量的weight decay对稳定大网络的训练过程有很大的帮忙（不仅仅是有regularizing作用）。对于包含大的全连接层的网络，如果不加weight decay的话，模型在训量的时候很容易发散。虽然可以通过适当的减小学习率来克服这一问题，但这无疑会降低训练速度。</p>
<h2 id="Unsupervised_and_semi-supervised_approaches"><a href="#Unsupervised_and_semi-supervised_approaches" class="headerlink" title="Unsupervised and semi-supervised approaches"></a>Unsupervised and semi-supervised approaches</h2><h3 id="1_uFF09Unsupervised_pre-training"><a href="#1_uFF09Unsupervised_pre-training" class="headerlink" title="1）Unsupervised pre-training"></a>1）Unsupervised pre-training</h3><p>由于test数据集很大，所以我们利用test数据集作为无监督学习的训练集来预训练网络。我们用<a href="http://link.springer.com/chapter/10.1007/978-3-642-21735-7_7" target="_blank" rel="external">CAE</a>来预训练网络的Convolution layers。</p>
<p>与文献的表述一致，我们发现预训练网络可以作为一个很好地regularizer（增大train loss，但是提高validation score）。但是测试时将validation集进行augmentation时，得到得结果却不尽人意，下面会更进一步的讨论。</p>
<p>预训练使得我们可以进一步的扩充我们的模型，但是由于预训练网络耗费的时间不止一点半点，所以在最终使用的模型中，我们没有使用预训练。</p>
<p>在实验预训练的过程中，为了学习到好的特征表示，我们更倾向于max-pooling以及unpooling layers来获取特征的稀疏化表示。我们没有尝试denoising autoencoder的原因主要有两个：首先，根据Masci的实验结果，max-pooling 和 unpooling方法提供了很好的filters，获得了比denoising autoencoder更好的performance，并且以后将这两个layers进行组合是很容易的；其次，参考denoising autoencoder网络的结构，速度会减慢很多。</p>
<p>以下是几种不同的开始预训练网络时的策略：</p>
<ul>
<li><strong>逐层贪心训练（greedy layerwise training） vs. 将deConvolutions连起来一起训练（training the full deconvolutional stack jointly，类似于end-to-end）：</strong>我们发现jointly的方式能够获得更好的performance，但有时候需要先进行逐层贪心训练来初始化网络后，jointly方式才会起作用。</li>
<li><strong>使用tied weights(using tied weights) vs. 不使用tied weights(using untied weights):</strong>将Convolution和deConvolution中的weights设置为互为转置矩阵能够使得Autoencoder训练的更快。因此，我们只试验了tied weights。</li>
</ul>
<p>我们在进行微调时也试验了一些不同的策略，我们发现如果保持相同的监督学习设置，随机初始化权重以及经过预训练初始化的网络在监督学习的performance上没有多大的差别。可能的原因是：在初始化dense layer的时候，可能dense layer的weights已经在一个合理的范围，使得在预训练阶段，Convolution layer会遗漏很多信息（feature）。</p>
<p>我们发现了两种方法来克服这个问题：</p>
<ul>
<li>暂时保持预训练层一段时间不变，仅仅训练dense layer（随机初始化）。如果只训练顶上几层网络，在反向传播的时候会非常快。</li>
<li>在Convolution layer部分将学习率减半。使随机初始化的dense layer更快的适应预训练后的Convolution layer。在dense layer获得比较好的weights range之前，Convolution layer进行监督学习的时候weights不会有大的变化。</li>
</ul>
<p>这两种方法得到的performance相差不大。</p>
<h3 id="3_29Pseudo-labeling"><a href="#3_29Pseudo-labeling" class="headerlink" title="3)Pseudo-labeling"></a>3)Pseudo-labeling</h3><p>另一种从测试集获取数据信息的方式就是将pseudo-labeling 和 knowledge distillation进行组合(<a href="http://arxiv.org/abs/1503.02531" target="_blank" rel="external">Distilling the Knowledge in a Neural Network</a>)。模型在采用Pseudo-labeling方法训练时得到的performance超出了我们的预期，所以我们详细的研究了一下这个方法。</p>
<p>Pseudo-labeling的做法是将test数据集加入到train数据集以得到一个更大的数据集。而原来test数据集对应的标签（即Pseudo-labeling），就是基于之前训练集训练好的模型进行预测得到的。这样做可以让我们去训练一个更大的网络，因为Pseudo-labeling有regularizing的作用。</p>
<p>我们试验了硬指标 (one-hot coded)和软指标(predicted probabilities)两种方式。但很快就决定采用软指标，因为它得到的performance要更好。</p>
<p>另一个重要的细节就是权衡原始train数据集和test数据集（也就是Pseudo-labeled data）在最终数据集中的比重。我们大多是时候采用的方法是：在一个mini-batch中Pseudo-labeled data占33%，原始的train数据集占67%。</p>
<p>也可以让Pseudo-labeled data占67%，这样模型的regularizing的程度会更大，我们甚至必须削弱dropout或者剔除，不然会造成underfitting。</p>
<p>我们的方法和knowledge distillation不同的地方是，我们使用test set而非train set来迁移特征信息。另一个不同的地方则是knowledge distillation可以让更小的，更快的模型获得和大模型一样的performance，而Pseudo-labeling则可以让我们建造更大的网络来获取更好的performance。</p>
<p>我们认为pseudo-labeling能提高performance的原因是获得的更大的数据集（train+test set）以及data augmentation和test-time augmentation（接下来会讨论）。将pseudo-labeling data加入到train data中时，可以说其包含了所有数据的特性（test+train），所以在训练的时候能够更好获取数据信息，得到更好的performance。</p>
<p>Pseudo-labeling获得最大的performance的提升是在最开始的时候（提升了0.015）。在后面，我们试验的bags模型才得到0.003-0.009的提升。</p>
<p><strong>本篇译文系作者原创，转载请先联系作者: 18254275587@163.com</strong></p>

            
          </span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2015/12/08/‎Summarize/DeepLearning资料汇总/" itemprop="url">
                  深度学习资料汇总
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2015-12-08T13:08:03+08:00" content="2015-12-08">
              2015-12-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Summarize/" itemprop="url" rel="index">
                    <span itemprop="name">Summarize</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody">
            
              <p>整理了平时自己用到的一些DL的资源</p>
<h2 id="u7F51_u7AD9"><a href="#u7F51_u7AD9" class="headerlink" title="网站"></a>网站</h2><ul>
<li><a href="http://deeplearning.net/" target="_blank" rel="external">deeplearning.net</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B" target="_blank" rel="external">UFLDL</a></li>
<li><a href="http://ml.memect.com/" target="_blank" rel="external">机器学习日报</a></li>
<li><a href="http://dataunion.org/14892.html" target="_blank" rel="external">kaggle winner solution</a></li>
<li><a href="http://ai.stanford.edu/courses/" target="_blank" rel="external">斯坦福人工智能实验室课程列表+ppt</a></li>
<li><a href="https://github.com/aikorea/awesome-rl" target="_blank" rel="external">强化学习资料汇总</a></li>
<li><a href="https://github.com/josephmisiti/awesome-machine-learning" target="_blank" rel="external">机器学习资料汇总</a></li>
<li><a href="http://valser.org/thread-748-1-1.html" target="_blank" rel="external">卷积网络Trick</a></li>
<li><a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html" target="_blank" rel="external">卷积网络Trick-English</a></li>
<li><a href="http://www.learnopencv.com/" target="_blank" rel="external">OpenCV(python/c++)</a></li>
</ul>
<h2 id="u4E66_u7C4D"><a href="#u4E66_u7C4D" class="headerlink" title="书籍"></a>书籍</h2><ul>
<li><a href="http://yunpan.cn/cFKrDrFyiIKpL" target="_blank" rel="external">Deep Learning-bengio</a>  访问密码 47ff</li>
<li><a href="http://yunpan.cn/cFKxhUx2tfwIQ" target="_blank" rel="external">Deep Learning Tutorial</a>  访问密码 e138</li>
<li><a href="http://yunpan.cn/cFKx7pLZ4seKQ" target="_blank" rel="external">Deep Learning :method and aplications</a>  访问密码 f238</li>
<li><a href="http://yunpan.cn/cFKYAukmctkMz" target="_blank" rel="external">PRML</a>  访问密码 b757</li>
<li><a href="http://yunpan.cn/cFKxGeDNGM4MM" target="_blank" rel="external">Deep Learning book</a>   访问密码 217f</li>
<li><a href="http://yunpan.cn/cFKYZGSUjvnud" target="_blank" rel="external">scikit-learn</a>  访问密码 4cfb</li>
<li><a href="http://yunpan.cn/cFKYPIBtzGEMy" target="_blank" rel="external">understanding ML</a>  访问密码 d946</li>
<li><a href="http://yunpan.cn/cFKq6DzYxckrB" target="_blank" rel="external">clustering</a>  访问密码 9a8a</li>
<li><a href="http://yunpan.cn/cFKq8xnbZjsxm" target="_blank" rel="external">Pattern Classification</a> 访问密码 1f3e</li>
</ul>
<h2 id="u5F00_u6E90_u5DE5_u5177"><a href="#u5F00_u6E90_u5DE5_u5177" class="headerlink" title="开源工具"></a>开源工具</h2><ul>
<li><a href="https://github.com/fchollet/keras" target="_blank" rel="external">keras</a>:基于theano的深度学习工具，方面拓展。</li>
<li><a href="https://github.com/dmlc/mxnet" target="_blank" rel="external">mxnet</a>:DMLC最新的一个深度学习的项目，比改进后的cxxnet还要好。</li>
<li><a href="https://github.com/jetpacapp/DeepBeliefSDK" target="_blank" rel="external">DeepBeliefSDK</a>:开源的CNN网络c++库，可以在ios，android，linux等系统的应用中调用。</li>
</ul>
<h2 id="u6709_u8DA3_u7684_u9879_u76EE"><a href="#u6709_u8DA3_u7684_u9879_u76EE" class="headerlink" title="有趣的项目"></a>有趣的项目</h2><ul>
<li><a href="https://github.com/jingpu/MDig" target="_blank" rel="external">MDig</a>:安卓的一个APP，可以识别图像中的0-9数字，用的CNN。</li>
</ul>
<p><strong>本文系作者原创，转载请先联系作者: 18254275587@163.com</strong></p>

            
          </span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2015/12/08/‎Summarize/kaggle-浮游生物分类--译文（三）/" itemprop="url">
                  kaggle-浮游生物分类比赛一等奖---译文（第三部分）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2015-12-08T13:08:03+08:00" content="2015-12-08">
              2015-12-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Summarize/" itemprop="url" rel="index">
                    <span itemprop="name">Summarize</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody">
            
              <p>接着<a href="http://meank.github.io/2015/10/09/%E2%80%8ESummarize/keggle-%E6%B5%AE%E6%B8%B8%E7%94%9F%E7%89%A9%E5%88%86%E7%B1%BB--%E8%AF%91%E6%96%87%EF%BC%88%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%89/" target="_blank" rel="external">上一篇</a>的内容</p>
<h2 id="model_averaging"><a href="#model_averaging" class="headerlink" title="model averaging"></a>model averaging</h2><p>我们将多个模型融合的结果作为最后提交的内容</p>
<h3 id="1_uFF09Test-time_augmentation"><a href="#1_uFF09Test-time_augmentation" class="headerlink" title="1）Test-time augmentation"></a>1）Test-time augmentation</h3><p>对于每一个单独的模型，我们使用不同的augmentation得到不同的预测结果，然后将这些结果融合，这对performence的提升有很大的帮助。我们将Test-time augmentation简写为TTA。</p>
<p>开始的时候，我们使用人工创造的仿射变换（affine transformations ）数据集应用于每一张图片来进行augmentation，这样比随机的选择参数进行augmentation得到的performence要更好。接下来，我们寻找更好的augmentation的方法，最终确定使用<a href="http://mathworld.wolfram.com/QuasirandomSequence.html" target="_blank" rel="external">quasi-random </a>进行70次转换，采用比训练时更加modest的参数。</p>
<p>使用TTA得到预测结果花费大约12小时。</p>
<h3 id="2_uFF09Finding_the_optimal_transformation_instead_of_averaging"><a href="#2_uFF09Finding_the_optimal_transformation_instead_of_averaging" class="headerlink" title="2）Finding the optimal transformation instead of averaging"></a>2）Finding the optimal transformation instead of averaging</h3><p>观察到上述TTA过程相对的提高了performence，我们在想有没有可能在预测的时候继续优化augmentation的参数。这是有可能的，因为仿射变换本身随着参数的不同而不同，也就是说具有可区分性。</p>
<p>为了做到这一点，我们将仿射变换（affine transformations ）作为网络的layer，这样的话网络在训练的时候，就能通过BP来更新augmentation的参数。但是求导有些复杂。</p>
<p>我们尝试了几个不同的方法来寻找优化的augmentation：</p>
<ul>
<li>优化augmentation的参数来最大化预测结果的置信度</li>
<li>训练一个卷积网络来预测augmentation的参数，然后传递给另外一个卷积网络使用。</li>
</ul>
<p>不幸的是这些方法都没有提升performence，我们在我们提交的结果中没有采用这个方法，但是我们以后会进一步的研究。</p>
<h3 id="3_uFF09Combining_different_models"><a href="#3_uFF09Combining_different_models" class="headerlink" title="3）Combining different models"></a>3）Combining different models</h3><p>我们总共训练的模型超过300个，所以我们必须从中选择好模型然后进行最终的融合。我们使用validation数据集来选择模型。我们优化模型的参数来减小模型在训练时的loss。</p>
<p>我们有规律的测试一些top-weights模型融合后的整体perforemence，然后在test数据集上进行预测。最后，大致的找到了融合模型的idea。</p>
<p>一旦融合模型选择好后，我们将他们均匀融合，或者用validation数据集来优化weighs。两种方法得到的performence不相上下。</p>
<p>选择要进行融合的模型并不一定是TTA过程中performence表现好的。一些performence很低也可以被选中，因为这些模型得到的预测结果与其他模型差异非常大。一些由于overfitting而performence低的模型也可以用来融合，因为模型的融合可以减小overfitting。</p>
<h3 id="4_uFF09Bagging"><a href="#4_uFF09Bagging" class="headerlink" title="4）Bagging"></a>4）Bagging</h3><p>为了更好的提升融合模型的performence，我们将其中的某些模型（大部分5个）用由不同的子数据集训练得到的模型代替。</p>
<h2 id="Miscellany"><a href="#Miscellany" class="headerlink" title="Miscellany"></a>Miscellany</h2><p>下面这些是我们尝试的其他一些方法，不同的方法有不同的提升效果：</p>
<ol>
<li>untied biases：提升的performence微乎其微</li>
<li>在FC层用winner take all nonlinearity （WTA）取代Relus/maxout。</li>
<li>smooth nonlinearities：用smooth nonlinearities取代LRelu，得到的performence更差。</li>
<li>specialist models：对于一些难以进行分类的类别，我们使用了额外的模型来训练，使用了包括 <a href="http://arxiv.org/abs/1503.02531" target="_blank" rel="external">knowledge distillation</a>和<a href="http://arxiv.org/abs/1412.6563" target="_blank" rel="external">self-informed neural network structure learning</a> 两种方法，但是都没有得到更好的performence。</li>
<li>batch normalization: 遗憾的是我们没有获得和<a href="http://arxiv.org/abs/1502.03167" target="_blank" rel="external">Ioffe and Szegedy </a>描述的一样的效果.</li>
<li>使用 <a href="http://arxiv.org/abs/1412.6630" target="_blank" rel="external">FaMe regularization</a> ，效果没有dropout好。</li>
<li>使用 <a href="http://arxiv.org/abs/1412.6596" target="_blank" rel="external">Reed</a>描述的 Semi-supervised learning 没有得到更好的performence，也没有降低overfitting。</li>
</ol>
<p>下面是一些我们测试能防止overfitting的一些方法（有遗漏）：</p>
<ul>
<li>dropout（传统以及变种）</li>
<li>好的 data augmentation</li>
<li>合适的模型结构（深度以及每一层的宽度都会影响overfitting）</li>
<li>weight-decay</li>
<li>无监督预训练（unsupervised pre-training）</li>
<li>cyclic pooling（特别是其中的 root-mean-square pooling）</li>
<li>leaky Relu</li>
<li>pseudo-labeling</li>
</ul>
<p>我们记录了比赛期间的准确率。最好的模型在validation上的准确率为82％,top-5准确率超过98％。这使得我们可以用模型作为工具加速人工注释．</p>
<p><a href="https://github.com/benanne/kaggle-ndsb" target="_blank" rel="external">code</a></p>
<p><strong>本篇译文系作者原创，转载请先联系作者: 18254275587@163.com</strong></p>

            
          </span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2015/12/08/UFLDL/DeepLearningbyAndrewNg---stackedautoencoder/" itemprop="url">
                  UFLDL-Stacked Autoencoder
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2015-12-08T13:08:02+08:00" content="2015-12-08">
              2015-12-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/UFLDL/" itemprop="url" rel="index">
                    <span itemprop="name">UFLDL</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody">
            
              <h2 id="When_should_we_use_fine-tuning_3F"><a href="#When_should_we_use_fine-tuning_3F" class="headerlink" title="When should we use fine-tuning?"></a>When should we use fine-tuning?</h2><p>It is typically used only if you have a large labeled training set; in this setting, fine-tuning can significantly improve the performance of your classifier. However, if you have a large unlabeled dataset (for unsupervised feature learning/pre-training) and only a relatively small labeled training set, then fine-tuning is significantly less likely to help.</p>
<h2 id="Stacked_Autoencoders_28Training_29_3A"><a href="#Stacked_Autoencoders_28Training_29_3A" class="headerlink" title="Stacked Autoencoders(Training):"></a>Stacked Autoencoders(Training):</h2><p>相当于用多个autoencoder去捕获输入集的特征。第一个autoencoder捕获了数据集的特征后，得到特征matrix1（hidden layer的权重）.然后将特征matrix1与输入集feedForward处理后的activation作为输入去捕获更高等级的特征matrix2（hidden layer的权重）.然后不断重复，再讲最后得到的特征activation作为输入集输入到softmax classifier（或者其他分类器）中训练。（<strong>注意并非将训练完后得到的特征matrix直接传给下一个autoencoder，而是将输入集与此输入集同级的特征matrix用feedForward方法得到的activation传入下一个autoencoder,即将输出传给下一个autoencode</strong>）。<br><img src="http://img.blog.csdn.net/20150408200903300" alt="获取第一层特征"><br><img src="http://img.blog.csdn.net/20150408200833788" alt="获取第二层特征"><br><img src="http://img.blog.csdn.net/20150408200935390" alt="训练分类器"><br>然后整个网络训练完之后，将各个步骤得到的特征matrix与分类器的参数合成新的网络。<br><img src="http://img.blog.csdn.net/20150408200557414" alt="训练完后组成的网络"></p>
<h2 id="fine-tuning_3A"><a href="#fine-tuning_3A" class="headerlink" title="fine-tuning:"></a>fine-tuning:</h2><p>其实就是将前面分步训练得到的hidden layer的Weight和softmax refression的softmaxTheta作为合成的神经网络的初始参数，然后运用神经网络的前馈和反向算法对初始参数进行微调（<strong>注意合成的网络是必须加上分类器的，不然也无法对神经网络的参数进行反向传播和微调（finetuning），此以softmax regression 为例</strong>）。具体可参考softmaxCost.m，sparseAutoencoderCost.m。</p>
<h2 id="u7EC3_u4E60_u9898_u7B54_u6848_uFF08_u63A8_u8350_u81EA_u5DF1_u5148_u8BD5_u7740_u5B8C_u6210_u540E_u53C2_u8003_uFF09_3A"><a href="#u7EC3_u4E60_u9898_u7B54_u6848_uFF08_u63A8_u8350_u81EA_u5DF1_u5148_u8BD5_u7740_u5B8C_u6210_u540E_u53C2_u8003_uFF09_3A" class="headerlink" title="练习题答案（推荐自己先试着完成后参考）:"></a>练习题答案（推荐自己先试着完成后参考）:</h2><p> -stackedAEExercise.m</p>
<figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% CS294A/CS294W Stacked Autoencoder Exercise</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%  Instructions</span></span><br><span class="line"><span class="comment">%  ------------</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">%  This file contains code that helps you get started on the</span></span><br><span class="line"><span class="comment">%  sstacked autoencoder exercise. You will need to complete code in</span></span><br><span class="line"><span class="comment">%  stackedAECost.m</span></span><br><span class="line"><span class="comment">%  You will also need to have implemented sparseAutoencoderCost.m and </span></span><br><span class="line"><span class="comment">%  softmaxCost.m from previous exercises. You will need the initializeParameters.m</span></span><br><span class="line"><span class="comment">%  loadMNISTImages.m, and loadMNISTLabels.m files from previous exercises.</span></span><br><span class="line"><span class="comment">%  </span></span><br><span class="line"><span class="comment">%  For the purpose of completing the assignment, you do not need to</span></span><br><span class="line"><span class="comment">%  change the code in this file. </span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 0: Here we provide the relevant parameters values that will</span></span><br><span class="line"><span class="comment">%  allow your sparse autoencoder to get good filters; you do not need to </span></span><br><span class="line"><span class="comment">%  change the parameters below.</span></span><br><span class="line"></span><br><span class="line"><span class="atom">inputSize</span> = <span class="number">28</span> * <span class="number">28</span>;</span><br><span class="line"><span class="atom">numClasses</span> = <span class="number">10</span>;</span><br><span class="line"><span class="atom">hiddenSizeL1</span> = <span class="number">200</span>;    <span class="comment">% Layer 1 Hidden Size</span></span><br><span class="line"><span class="atom">hiddenSizeL2</span> = <span class="number">200</span>;    <span class="comment">% Layer 2 Hidden Size</span></span><br><span class="line"><span class="atom">sparsityParam</span> = <span class="number">0.1</span>;   <span class="comment">% desired average activation of the hidden units.</span></span><br><span class="line">                       <span class="comment">% (This was denoted by the Greek alphabet rho, which looks like a lower-case "p",</span></span><br><span class="line">		               <span class="comment">%  in the lecture notes). </span></span><br><span class="line"><span class="atom">lambda</span> = <span class="number">3e-3</span>;         <span class="comment">% weight decay parameter       </span></span><br><span class="line"><span class="atom">beta</span> = <span class="number">3</span>;              <span class="comment">% weight of sparsity penalty term       </span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 1: Load data from the MNIST database</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%  This loads our training data from the MNIST database files.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Load MNIST database files</span></span><br><span class="line"><span class="atom">trainData</span> = <span class="atom">loadMNISTImages</span>(<span class="string">'mnist/train-images-idx3-ubyte'</span>);</span><br><span class="line"><span class="atom">trainLabels</span> = <span class="atom">loadMNISTLabels</span>(<span class="string">'mnist/train-labels-idx1-ubyte'</span>);</span><br><span class="line"></span><br><span class="line"><span class="atom">trainLabels</span>(<span class="atom">trainLabels</span> == <span class="number">0</span>) = <span class="number">10</span>; <span class="comment">% Remap 0 to 10 since our labels need to start from 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 2: Train the first sparse autoencoder</span></span><br><span class="line"><span class="comment">%  This trains the first sparse autoencoder on the unlabelled STL training</span></span><br><span class="line"><span class="comment">%  images.</span></span><br><span class="line"><span class="comment">%  If you've correctly implemented sparseAutoencoderCost.m, you don't need</span></span><br><span class="line"><span class="comment">%  to change anything here.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%  Randomly initialize the parameters</span></span><br><span class="line"><span class="atom">sae1Theta</span> = <span class="atom">initializeParameters</span>(<span class="atom">hiddenSizeL1</span>, <span class="atom">inputSize</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------------------- YOUR CODE HERE  ---------------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Train the first layer sparse autoencoder, this layer has</span></span><br><span class="line"><span class="comment">%                an hidden size of "hiddenSizeL1"</span></span><br><span class="line"><span class="comment">%                You should store the optimal parameters in sae1OptTheta</span></span><br><span class="line"><span class="atom">addpath</span> <span class="atom">minFunc</span>/</span><br><span class="line"><span class="atom">options</span>.<span class="name">Method</span> = <span class="string">'lbfgs'</span>; <span class="comment">% Here, we use L-BFGS to optimize our cost</span></span><br><span class="line">                          <span class="comment">% function. Generally, for minFunc to work, you</span></span><br><span class="line">                          <span class="comment">% need a function pointer with two outputs: the</span></span><br><span class="line">                          <span class="comment">% function value and the gradient. In our problem,</span></span><br><span class="line">                          <span class="comment">% sparseAutoencoderCost.m satisfies this.</span></span><br><span class="line"><span class="atom">options</span>.<span class="atom">maxIter</span> = <span class="number">40</span>;	  <span class="comment">% Maximum number of iterations of L-BFGS to run </span></span><br><span class="line"><span class="atom">options</span>.<span class="atom">display</span> = <span class="string">'on'</span>;</span><br><span class="line">[<span class="atom">sae1OptTheta</span>, <span class="atom">cost</span>] = <span class="atom">minFunc</span>( @(<span class="atom">p</span>) <span class="atom">sparseAutoencoderCost</span>(<span class="atom">p</span>, ...</span><br><span class="line">                                   <span class="atom">inputSize</span>, <span class="atom">hiddenSizeL1</span>, ...</span><br><span class="line">                                   <span class="atom">lambda</span>, <span class="atom">sparsityParam</span>, ...</span><br><span class="line">                                   <span class="atom">beta</span>, <span class="atom">trainData</span>), ...</span><br><span class="line">                              <span class="atom">sae1Theta</span> , <span class="atom">options</span>);</span><br><span class="line"><span class="comment">% -------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 2: Train the second sparse autoencoder</span></span><br><span class="line"><span class="comment">%  This trains the second sparse autoencoder on the first autoencoder</span></span><br><span class="line"><span class="comment">%  featurse.</span></span><br><span class="line"><span class="comment">%  If you've correctly implemented sparseAutoencoderCost.m, you don't need</span></span><br><span class="line"><span class="comment">%  to change anything here.</span></span><br><span class="line"></span><br><span class="line">[<span class="atom">sae1Features</span>] = <span class="atom">feedForwardAutoencoder</span>(<span class="atom">sae1OptTheta</span>, <span class="atom">hiddenSizeL1</span>, ...</span><br><span class="line">                                        <span class="atom">inputSize</span>, <span class="atom">trainData</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">%  Randomly initialize the parameters</span></span><br><span class="line"><span class="atom">sae2Theta</span> = <span class="atom">initializeParameters</span>(<span class="atom">hiddenSizeL2</span>, <span class="atom">hiddenSizeL1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------------------- YOUR CODE HERE  ---------------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Train the second layer sparse autoencoder, this layer has</span></span><br><span class="line"><span class="comment">%                an hidden size of "hiddenSizeL2" and an inputsize of</span></span><br><span class="line"><span class="comment">%                "hiddenSizeL1"</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%                You should store the optimal parameters in sae2OptTheta</span></span><br><span class="line"><span class="atom">addpath</span> <span class="atom">minFunc</span>/</span><br><span class="line"><span class="atom">options</span>.<span class="name">Method</span> = <span class="string">'lbfgs'</span>; <span class="comment">% Here, we use L-BFGS to optimize our cost</span></span><br><span class="line">                          <span class="comment">% function. Generally, for minFunc to work, you</span></span><br><span class="line">                          <span class="comment">% need a function pointer with two outputs: the</span></span><br><span class="line">                          <span class="comment">% function value and the gradient. In our problem,</span></span><br><span class="line">                          <span class="comment">% sparseAutoencoderCost.m satisfies this.</span></span><br><span class="line"><span class="atom">options</span>.<span class="atom">maxIter</span> = <span class="number">40</span>;	  <span class="comment">% Maximum number of iterations of L-BFGS to run </span></span><br><span class="line"><span class="atom">options</span>.<span class="atom">display</span> = <span class="string">'on'</span>;</span><br><span class="line">[<span class="atom">sae2OptTheta</span>, <span class="atom">cost</span>] = <span class="atom">minFunc</span>( @(<span class="atom">p</span>) <span class="atom">sparseAutoencoderCost</span>(<span class="atom">p</span>, ...</span><br><span class="line">                                   <span class="atom">hiddenSizeL1</span>, <span class="atom">hiddenSizeL2</span>, ...</span><br><span class="line">                                   <span class="atom">lambda</span>, <span class="atom">sparsityParam</span>, ...</span><br><span class="line">                                   <span class="atom">beta</span>, <span class="atom">sae1Features</span>), ...</span><br><span class="line">                              <span class="atom">sae2Theta</span> , <span class="atom">options</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 3: Train the softmax classifier</span></span><br><span class="line"><span class="comment">%  This trains the sparse autoencoder on the second autoencoder features.</span></span><br><span class="line"><span class="comment">%  If you've correctly implemented softmaxCost.m, you don't need</span></span><br><span class="line"><span class="comment">%  to change anything here.</span></span><br><span class="line"></span><br><span class="line">[<span class="atom">sae2Features</span>] = <span class="atom">feedForwardAutoencoder</span>(<span class="atom">sae2OptTheta</span>, <span class="atom">hiddenSizeL2</span>, ...</span><br><span class="line">                                        <span class="atom">hiddenSizeL1</span>, <span class="atom">sae1Features</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">%  Randomly initialize the parameters</span></span><br><span class="line"><span class="atom">saeSoftmaxTheta</span> = <span class="number">0.005</span> * <span class="atom">randn</span>(<span class="atom">hiddenSizeL2</span> * <span class="atom">numClasses</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------------------- YOUR CODE HERE  ---------------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Train the softmax classifier, the classifier takes in</span></span><br><span class="line"><span class="comment">%                input of dimension "hiddenSizeL2" corresponding to the</span></span><br><span class="line"><span class="comment">%                hidden layer size of the 2nd layer.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%                You should store the optimal parameters in saeSoftmaxOptTheta </span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%  NOTE: If you used softmaxTrain to complete this part of the exercise,</span></span><br><span class="line"><span class="comment">%        set saeSoftmaxOptTheta = softmaxModel.optTheta(:);</span></span><br><span class="line"><span class="atom">options</span>.<span class="atom">maxIter</span> = <span class="number">100</span>;</span><br><span class="line"><span class="atom">softmaxModel</span> = <span class="atom">softmaxTrain</span>(<span class="atom">hiddenSizeL2</span>, <span class="number">10</span>, <span class="atom">lambda</span>, ...</span><br><span class="line">                           <span class="atom">sae2Features</span>, <span class="atom">trainLabels</span> , <span class="atom">options</span>);</span><br><span class="line"><span class="atom">saeSoftmaxOptTheta</span> = <span class="atom">softmaxModel</span>.<span class="atom">optTheta</span>(:);</span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 5: Finetune softmax model</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Implement the stackedAECost to give the combined cost of the whole model</span></span><br><span class="line"><span class="comment">% then run this cell.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize the stack using the parameters learned</span></span><br><span class="line"><span class="atom">stack</span> = <span class="atom">cell</span>(<span class="number">2</span>,<span class="number">1</span>);</span><br><span class="line"><span class="atom">stack</span>&#123;<span class="number">1</span>&#125;.<span class="atom">w</span> = <span class="atom">reshape</span>(<span class="atom">sae1OptTheta</span>(<span class="number">1</span>:<span class="atom">hiddenSizeL1</span>*<span class="atom">inputSize</span>), ...</span><br><span class="line">                     <span class="atom">hiddenSizeL1</span>, <span class="atom">inputSize</span>);</span><br><span class="line"><span class="atom">stack</span>&#123;<span class="number">1</span>&#125;.<span class="atom">b</span> = <span class="atom">sae1OptTheta</span>(<span class="number">2</span>*<span class="atom">hiddenSizeL1</span>*<span class="atom">inputSize</span>+<span class="number">1</span>:<span class="number">2</span>*<span class="atom">hiddenSizeL1</span>*<span class="atom">inputSize</span>+<span class="atom">hiddenSizeL1</span>);</span><br><span class="line"><span class="atom">stack</span>&#123;<span class="number">2</span>&#125;.<span class="atom">w</span> = <span class="atom">reshape</span>(<span class="atom">sae2OptTheta</span>(<span class="number">1</span>:<span class="atom">hiddenSizeL2</span>*<span class="atom">hiddenSizeL1</span>), ...</span><br><span class="line">                     <span class="atom">hiddenSizeL2</span>, <span class="atom">hiddenSizeL1</span>);</span><br><span class="line"><span class="atom">stack</span>&#123;<span class="number">2</span>&#125;.<span class="atom">b</span> = <span class="atom">sae2OptTheta</span>(<span class="number">2</span>*<span class="atom">hiddenSizeL2</span>*<span class="atom">hiddenSizeL1</span>+<span class="number">1</span>:<span class="number">2</span>*<span class="atom">hiddenSizeL2</span>*<span class="atom">hiddenSizeL1</span>+<span class="atom">hiddenSizeL2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize the parameters for the deep model</span></span><br><span class="line">[<span class="atom">stackparams</span>, <span class="atom">netconfig</span>] = <span class="atom">stack2params</span>(<span class="atom">stack</span>);</span><br><span class="line"><span class="atom">stackedAETheta</span> = [ <span class="atom">saeSoftmaxOptTheta</span> ; <span class="atom">stackparams</span> ];</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------------------- YOUR CODE HERE  ---------------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Train the deep network, hidden size here refers to the '</span></span><br><span class="line"><span class="comment">%                dimension of the input to the classifier, which corresponds </span></span><br><span class="line"><span class="comment">%                to "hiddenSizeL2".</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------------------------------------------------------------</span></span><br><span class="line">[<span class="atom">stackedAEOptTheta</span>, <span class="atom">cost</span>] =  <span class="atom">minFunc</span>(@(<span class="atom">p</span>)<span class="atom">stackedAECost</span>(<span class="atom">p</span>,<span class="atom">inputSize</span>,<span class="atom">hiddenSizeL2</span>,...</span><br><span class="line">                         <span class="atom">numClasses</span>, <span class="atom">netconfig</span>,<span class="atom">lambda</span>, <span class="atom">trainData</span>, <span class="atom">trainLabels</span>),...</span><br><span class="line">                        <span class="atom">stackedAETheta</span>,<span class="atom">options</span>);</span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 6: Test </span></span><br><span class="line"><span class="comment">%  Instructions: You will need to complete the code in stackedAEPredict.m</span></span><br><span class="line"><span class="comment">%                before running this part of the code</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Get labelled test images</span></span><br><span class="line"><span class="comment">% Note that we apply the same kind of preprocessing as the training set</span></span><br><span class="line"><span class="atom">testData</span> = <span class="atom">loadMNISTImages</span>(<span class="string">'mnist/t10k-images-idx3-ubyte'</span>);</span><br><span class="line"><span class="atom">testLabels</span> = <span class="atom">loadMNISTLabels</span>(<span class="string">'mnist/t10k-labels-idx1-ubyte'</span>);</span><br><span class="line"></span><br><span class="line"><span class="atom">testLabels</span>(<span class="atom">testLabels</span> == <span class="number">0</span>) = <span class="number">10</span>; <span class="comment">% Remap 0 to 10</span></span><br><span class="line"></span><br><span class="line">[<span class="atom">pred</span>] = <span class="atom">stackedAEPredict</span>(<span class="atom">stackedAETheta</span>, <span class="atom">inputSize</span>, <span class="atom">hiddenSizeL2</span>, ...</span><br><span class="line">                          <span class="atom">numClasses</span>, <span class="atom">netconfig</span>, <span class="atom">testData</span>);</span><br><span class="line"></span><br><span class="line"><span class="atom">acc</span> = <span class="atom">mean</span>(<span class="atom">testLabels</span>(:) == <span class="atom">pred</span>(:));</span><br><span class="line"><span class="atom">fprintf</span>(<span class="string">'Before Finetuning Test Accuracy: %0.3f%%\n'</span>, <span class="atom">acc</span> * <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line">[<span class="atom">pred</span>] = <span class="atom">stackedAEPredict</span>(<span class="atom">stackedAEOptTheta</span>, <span class="atom">inputSize</span>, <span class="atom">hiddenSizeL2</span>, ...</span><br><span class="line">                          <span class="atom">numClasses</span>, <span class="atom">netconfig</span>, <span class="atom">testData</span>);</span><br><span class="line"></span><br><span class="line"><span class="atom">acc</span> = <span class="atom">mean</span>(<span class="atom">testLabels</span>(:) == <span class="atom">pred</span>(:));</span><br><span class="line"><span class="atom">fprintf</span>(<span class="string">'After Finetuning Test Accuracy: %0.3f%%\n'</span>, <span class="atom">acc</span> * <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Accuracy is the proportion of correctly classified images</span></span><br><span class="line"><span class="comment">% The results for our implementation were:</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Before Finetuning Test Accuracy: 87.7%</span></span><br><span class="line"><span class="comment">% After Finetuning Test Accuracy:  97.6%</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% If your values are too low (accuracy less than 95%), you should check </span></span><br><span class="line"><span class="comment">% your code for errors, and make sure you are training on the </span></span><br><span class="line"><span class="comment">% entire data set of 60000 28x28 training images </span></span><br><span class="line"><span class="comment">% (unless you modified the loading code, this should be the case)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>stackedAECost.m</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[ cost, grad ]</span> = <span class="title">stackedAECost</span><span class="params">(theta, inputSize, hiddenSize, ...</span><br><span class="line">                                              numClasses, netconfig, ...</span><br><span class="line">                                              lambda, data, labels)</span></span></span><br><span class="line">                                         </span><br><span class="line"><span class="comment">% stackedAECost: Takes a trained softmaxTheta and a training data set with labels,</span></span><br><span class="line"><span class="comment">% and returns cost and gradient using a stacked autoencoder model. Used for</span></span><br><span class="line"><span class="comment">% finetuning.</span></span><br><span class="line">                                         </span><br><span class="line"><span class="comment">% theta: trained weights from the autoencoder</span></span><br><span class="line"><span class="comment">% visibleSize: the number of input units</span></span><br><span class="line"><span class="comment">% hiddenSize:  the number of hidden units *at the 2nd layer*</span></span><br><span class="line"><span class="comment">% numClasses:  the number of categories</span></span><br><span class="line"><span class="comment">% netconfig:   the network configuration of the stack</span></span><br><span class="line"><span class="comment">% lambda:      the weight regularization penalty</span></span><br><span class="line"><span class="comment">% data: Our matrix containing the training data as columns.  So, data(:,i) is the i-th training example. </span></span><br><span class="line"><span class="comment">% labels: A vector containing labels, where labels(i) is the label for the</span></span><br><span class="line"><span class="comment">% i-th training example</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% Unroll softmaxTheta parameter</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% We first extract the part which compute the softmax gradient</span></span><br><span class="line">softmaxTheta = <span class="built_in">reshape</span>(theta(<span class="number">1</span>:hiddenSize*numClasses), numClasses, hiddenSize);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Extract out the "stack"</span></span><br><span class="line">stack = params2stack(theta(hiddenSize*numClasses+<span class="number">1</span>:<span class="keyword">end</span>), netconfig);</span><br><span class="line"></span><br><span class="line"><span class="comment">% You will need to compute the following gradients</span></span><br><span class="line">softmaxThetaGrad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(softmaxTheta));</span><br><span class="line">stackgrad = cell(<span class="built_in">size</span>(stack));</span><br><span class="line"><span class="keyword">for</span> delta = <span class="number">1</span>:<span class="built_in">numel</span>(stack)</span><br><span class="line">    stackgrad<span class="cell">&#123;delta&#125;</span>.w = <span class="built_in">zeros</span>(<span class="built_in">size</span>(stack<span class="cell">&#123;delta&#125;</span>.w));</span><br><span class="line">    stackgrad<span class="cell">&#123;delta&#125;</span>.b = <span class="built_in">zeros</span>(<span class="built_in">size</span>(stack<span class="cell">&#123;delta&#125;</span>.b));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">cost = <span class="number">0</span>; <span class="comment">% You need to compute this</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% You might find these variables useful</span></span><br><span class="line">M = <span class="built_in">size</span>(data, <span class="number">2</span>);</span><br><span class="line">groundTruth = full(sparse(labels, <span class="number">1</span>:M, <span class="number">1</span>));<span class="comment">%input labels</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% --------------------------- YOUR CODE HERE -----------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Compute the cost function and gradient vector for </span></span><br><span class="line"><span class="comment">%                the stacked autoencoder.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%                You are given a stack variable which is a cell-array of</span></span><br><span class="line"><span class="comment">%                the weights and biases for every layer. In particular, you</span></span><br><span class="line"><span class="comment">%                can refer to the weights of Layer d, using stack&#123;d&#125;.w and</span></span><br><span class="line"><span class="comment">%                the biases using stack&#123;d&#125;.b . To get the total number of</span></span><br><span class="line"><span class="comment">%                layers, you can use numel(stack).</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%                The last layer of the network is connected to the softmax</span></span><br><span class="line"><span class="comment">%                classification layer, softmaxTheta.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%                You should compute the gradients for the softmaxTheta,</span></span><br><span class="line"><span class="comment">%                storing that in softmaxThetaGrad. Similarly, you should</span></span><br><span class="line"><span class="comment">%                compute the gradients for each layer in the stack, storing</span></span><br><span class="line"><span class="comment">%                the gradients in stackgrad&#123;d&#125;.w and stackgrad&#123;d&#125;.b</span></span><br><span class="line"><span class="comment">%                Note that the size of the matrices in stackgrad should</span></span><br><span class="line"><span class="comment">%                match exactly that of the size of the matrices in stack.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------------------------------------------------------------</span></span><br><span class="line">depth = <span class="built_in">numel</span>(stack)<span class="comment">% 神经网络的层数(不包括softmax层)</span></span><br><span class="line"></span><br><span class="line">z = cell(depth+<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">a = cell(depth+<span class="number">1</span>,<span class="number">1</span>);<span class="comment">%进行前馈神经网络计算所需要的参数</span></span><br><span class="line"></span><br><span class="line">a<span class="cell">&#123;<span class="number">1</span>&#125;</span> =data;<span class="comment">%输入层</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index = <span class="number">1</span>:depth<span class="comment">%前馈神经网络计算(为什么要加1)</span></span><br><span class="line">    z<span class="cell">&#123;index+<span class="number">1</span>&#125;</span> =  stack<span class="cell">&#123;index&#125;</span>.w*a<span class="cell">&#123;index&#125;</span>+<span class="built_in">repmat</span>(stack<span class="cell">&#123;index&#125;</span>.b, <span class="number">1</span>, <span class="built_in">size</span>(a<span class="cell">&#123;index&#125;</span>,<span class="number">2</span>));</span><br><span class="line">    a<span class="cell">&#123;index+<span class="number">1</span>&#125;</span> = sigmoid(z<span class="cell">&#123;index+<span class="number">1</span>&#125;</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">model = softmaxTheta*a<span class="cell">&#123;depth+<span class="number">1</span>&#125;</span>; <span class="comment">%神经网络最后一层的activation传入softmax regression。</span></span><br><span class="line">model = <span class="built_in">bsxfun</span>(@minus, model , max(model , <span class="matrix">[]</span>, <span class="number">1</span>));  </span><br><span class="line">h = <span class="built_in">exp</span>(model );</span><br><span class="line">h =  <span class="built_in">bsxfun</span>(@rdivide, h, sum(h));  </span><br><span class="line"><span class="built_in">size</span>(groundTruth);</span><br><span class="line">cost = -<span class="number">1</span>/numClasses*sum(sum(groundTruth.*<span class="built_in">log</span>(h)))+lambda/<span class="number">2</span>*sum(sum(softmaxTheta.^<span class="number">2</span>));</span><br><span class="line">softmaxThetaGrad = -<span class="number">1</span>/numClasses*((groundTruth-h)*a<span class="cell">&#123;depth+<span class="number">1</span>&#125;</span><span class="operator">'</span>)+lambda*softmaxTheta;</span><br><span class="line"></span><br><span class="line"><span class="comment">%反向传播算法</span></span><br><span class="line">delta = cell(depth+<span class="number">1</span>);</span><br><span class="line"><span class="comment">%I is the input labels and P is the vector of conditional probabilities.</span></span><br><span class="line">delta<span class="cell">&#123;depth+<span class="number">1</span>&#125;</span> = -(softmaxTheta<span class="operator">'</span> * (groundTruth - h)) .* a<span class="cell">&#123;depth+<span class="number">1</span>&#125;</span> .* (<span class="number">1</span>-a<span class="cell">&#123;depth+<span class="number">1</span>&#125;</span>);</span><br><span class="line"><span class="keyword">for</span> layer = (depth:-<span class="number">1</span>:<span class="number">2</span>)</span><br><span class="line">  delta<span class="cell">&#123;layer&#125;</span> = (stack<span class="cell">&#123;layer&#125;</span>.w<span class="operator">'</span> * delta<span class="cell">&#123;layer+<span class="number">1</span>&#125;</span>) .* a<span class="cell">&#123;layer&#125;</span> .* (<span class="number">1</span>-a<span class="cell">&#123;layer&#125;</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">for</span> layer = (depth:-<span class="number">1</span>:<span class="number">1</span>)</span><br><span class="line">  stackgrad<span class="cell">&#123;layer&#125;</span>.w = (<span class="number">1</span>/numClasses) * delta<span class="cell">&#123;layer+<span class="number">1</span>&#125;</span> * a<span class="cell">&#123;layer&#125;</span><span class="operator">'</span>;</span><br><span class="line">  stackgrad<span class="cell">&#123;layer&#125;</span>.b = (<span class="number">1</span>/numClasses) * sum(delta<span class="cell">&#123;layer+<span class="number">1</span>&#125;</span>, <span class="number">2</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%% Roll gradient vector</span></span><br><span class="line">grad = <span class="matrix">[softmaxThetaGrad(:) ; stack2params(stackgrad)]</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">% You might find this useful</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sigm</span> = <span class="title">sigmoid</span><span class="params">(x)</span></span></span><br><span class="line">    sigm = <span class="number">1</span> ./ (<span class="number">1</span> + <span class="built_in">exp</span>(-x));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ul>
<li>stackedAEPredict.m</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[pred]</span> = <span class="title">stackedAEPredict</span><span class="params">(theta, inputSize, hiddenSize, numClasses, netconfig, data)</span></span></span><br><span class="line">                                         </span><br><span class="line"><span class="comment">% stackedAEPredict: Takes a trained theta and a test data set,</span></span><br><span class="line"><span class="comment">% and returns the predicted labels for each example.</span></span><br><span class="line">                                         </span><br><span class="line"><span class="comment">% theta: trained weights from the autoencoder</span></span><br><span class="line"><span class="comment">% visibleSize: the number of input units</span></span><br><span class="line"><span class="comment">% hiddenSize:  the number of hidden units *at the 2nd layer*</span></span><br><span class="line"><span class="comment">% numClasses:  the number of categories</span></span><br><span class="line"><span class="comment">% data: Our matrix containing the training data as columns.  So, data(:,i) is the i-th training example. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Your code should produce the prediction matrix </span></span><br><span class="line"><span class="comment">% pred, where pred(i) is argmax_c P(y(c) | x(i)).</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">%% Unroll theta parameter</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% We first extract the part which compute the softmax gradient</span></span><br><span class="line">softmaxTheta = <span class="built_in">reshape</span>(theta(<span class="number">1</span>:hiddenSize*numClasses), numClasses, hiddenSize);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Extract out the "stack"</span></span><br><span class="line">stack = params2stack(theta(hiddenSize*numClasses+<span class="number">1</span>:<span class="keyword">end</span>), netconfig);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------- YOUR CODE HERE --------------------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Compute pred using theta assuming that the labels start </span></span><br><span class="line"><span class="comment">%                from 1.</span></span><br><span class="line"><span class="comment">%前馈神经网络算法</span></span><br><span class="line">depth = <span class="built_in">numel</span>(stack);</span><br><span class="line">z = cell(depth+<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">a = cell(depth+<span class="number">1</span>, <span class="number">1</span>);</span><br><span class="line">a<span class="cell">&#123;<span class="number">1</span>&#125;</span> = data;</span><br><span class="line"><span class="keyword">for</span> layer = (<span class="number">1</span>:depth)</span><br><span class="line">  z<span class="cell">&#123;layer+<span class="number">1</span>&#125;</span> = stack<span class="cell">&#123;layer&#125;</span>.w * a<span class="cell">&#123;layer&#125;</span> + <span class="built_in">repmat</span>(stack<span class="cell">&#123;layer&#125;</span>.b, <span class="matrix">[<span class="number">1</span>, size(a&#123;layer&#125;,<span class="number">2</span>)]</span>);</span><br><span class="line">  a<span class="cell">&#123;layer+<span class="number">1</span>&#125;</span> = sigmoid(z<span class="cell">&#123;layer+<span class="number">1</span>&#125;</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="matrix">[index, pred]</span> = max(softmaxTheta * a<span class="cell">&#123;depth+<span class="number">1</span>&#125;</span>);<span class="comment">%预测</span></span><br><span class="line"><span class="comment">% -----------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">% You might find this useful</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sigm</span> = <span class="title">sigmoid</span><span class="params">(x)</span></span></span><br><span class="line">    sigm = <span class="number">1</span> ./ (<span class="number">1</span> + <span class="built_in">exp</span>(-x));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>

            
          </span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2015/12/08/UFLDL/DeepLearningbyAndrewNg---Sparsecoding/" itemprop="url">
                  UFLDL-Sparse coding
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2015-12-08T13:08:02+08:00" content="2015-12-08">
              2015-12-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/UFLDL/" itemprop="url" rel="index">
                    <span itemprop="name">UFLDL</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody">
            
              <h2 id="u7A00_u758F_u7F16_u7801_u4ECB_u7ECD"><a href="#u7A00_u758F_u7F16_u7801_u4ECB_u7ECD" class="headerlink" title="稀疏编码介绍"></a>稀疏编码介绍</h2><p><img src="http://i.imgur.com/pYpMxtv.png" alt=""></p>
<h2 id="u5B66_u4E60_u7B97_u6CD5"><a href="#u5B66_u4E60_u7B97_u6CD5" class="headerlink" title="学习算法"></a>学习算法</h2><p><img src="http://i.imgur.com/wZQVbxE.png" alt=""></p>
<h2 id="u76EE_u6807_u51FD_u6570"><a href="#u76EE_u6807_u51FD_u6570" class="headerlink" title="目标函数"></a>目标函数</h2><p><img src="http://i.imgur.com/61mb5w7.png" alt=""></p>
<h2 id="u62D3_u6251_u7A00_u758F_u7F16_u7801"><a href="#u62D3_u6251_u7A00_u758F_u7F16_u7801" class="headerlink" title="拓扑稀疏编码"></a>拓扑稀疏编码</h2><p><img src="http://i.imgur.com/zWZhpkL.png" alt=""></p>
<h2 id="u7A00_u758F_u7F16_u7801_u5B9E_u8DF5_uFF08_u6280_u5DE7_uFF09"><a href="#u7A00_u758F_u7F16_u7801_u5B9E_u8DF5_uFF08_u6280_u5DE7_uFF09" class="headerlink" title="稀疏编码实践（技巧）"></a>稀疏编码实践（技巧）</h2><p><img src="http://i.imgur.com/aAsGOlK.png" alt=""></p>
<h2 id="u4F5C_u4E1A_u9898"><a href="#u4F5C_u4E1A_u9898" class="headerlink" title="作业题"></a>作业题</h2><p>该作业需要我们自己对costfunction求导：<br><img src="http://img.blog.csdn.net/20150419161119486" alt="这里写图片描述"><br><img src="http://img.blog.csdn.net/20150419161256150" alt="这里写图片描述"></p>

            
          </span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2015/12/08/UFLDL/DeepLearningbyAndrewNg---Softmaxregression/" itemprop="url">
                  UFLDL-Softmax
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2015-12-08T13:08:01+08:00" content="2015-12-08">
              2015-12-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/UFLDL/" itemprop="url" rel="index">
                    <span itemprop="name">UFLDL</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody">
            
              <p>这是UFLDL的编程练习。</p>
<h2 id="Weight_decay_uFF08Softmax__u56DE_u5F52_u6709_u4E00_u4E2A_u4E0D_u5BFB_u5E38_u7684_u7279_u70B9_uFF1A_u5B83_u6709_u4E00_u4E2A_u201C_u5197_u4F59_u201D_u7684_u53C2_u6570_u96C6_uFF09_u540E_u7684cost_function_u548C_u68AF_u5EA6_u51FD_u6570_uFF1A"><a href="#Weight_decay_uFF08Softmax__u56DE_u5F52_u6709_u4E00_u4E2A_u4E0D_u5BFB_u5E38_u7684_u7279_u70B9_uFF1A_u5B83_u6709_u4E00_u4E2A_u201C_u5197_u4F59_u201D_u7684_u53C2_u6570_u96C6_uFF09_u540E_u7684cost_function_u548C_u68AF_u5EA6_u51FD_u6570_uFF1A" class="headerlink" title="Weight decay（Softmax 回归有一个不寻常的特点：它有一个“冗余”的参数集）后的cost function和梯度函数："></a>Weight decay（Softmax 回归有一个不寻常的特点：它有一个“冗余”的参数集）后的cost function和梯度函数：</h2><p><img src="http://i.imgur.com/gwLtxyi.png" alt=""></p>
<h2 id="bsxfun_u51FD_u6570_u7684_u4F7F_u7528_uFF1A"><a href="#bsxfun_u51FD_u6570_u7684_u4F7F_u7528_uFF1A" class="headerlink" title="bsxfun函数的使用："></a>bsxfun函数的使用：</h2><p><img src="http://i.imgur.com/6nlMDfI.png" alt=""></p>
<h2 id="u7EC3_u4E60_u9898_u7B54_u6848_uFF08_u5EFA_u8BAE_u81EA_u5DF1_u5B8C_u6210_uFF0C_u540E_u53C2_u8003_uFF09_uFF1A"><a href="#u7EC3_u4E60_u9898_u7B54_u6848_uFF08_u5EFA_u8BAE_u81EA_u5DF1_u5B8C_u6210_uFF0C_u540E_u53C2_u8003_uFF09_uFF1A" class="headerlink" title="练习题答案（建议自己完成，后参考）："></a>练习题答案（建议自己完成，后参考）：</h2><ul>
<li>softmaxCost.m:</li>
</ul>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">M</span> = theta*data; %<span class="literal">exp</span>(theta(<span class="keyword">l</span>)' * x(i))</span><br><span class="line"><span class="keyword">M</span> = bsxfun(@minus, <span class="keyword">M</span>, <span class="literal">max</span>(<span class="keyword">M</span>, [], 1));  </span><br><span class="line"><span class="keyword">h</span> = <span class="literal">exp</span>(<span class="keyword">M</span>);</span><br><span class="line"><span class="keyword">h</span> =  bsxfun(@rdivide, <span class="keyword">h</span>, <span class="literal">sum</span>(<span class="keyword">h</span>));  </span><br><span class="line">size(groundTruth);</span><br><span class="line">cost = -1/numCases*<span class="literal">sum</span>(<span class="literal">sum</span>(groundTruth.*<span class="literal">log</span>(<span class="keyword">h</span>)))+lambda/2*<span class="literal">sum</span>(<span class="literal">sum</span>(theta.^2));  </span><br><span class="line">thetagrad = -1/numCases*((groundTruth-<span class="keyword">h</span>)*data')+lambda*theta;</span><br></pre></td></tr></table></figure>
<ul>
<li>softPredict.m:</li>
</ul>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="built_in">index</span> ,  pred]= <span class="built_in">max</span>(theta * <span class="type">data</span>,[],<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

            
          </span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2015/12/08/UFLDL/DeeplearningbyAndrewNg---LinearDecoder/" itemprop="url">
                  UFLDL-Linear Decoder
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2015-12-08T13:08:00+08:00" content="2015-12-08">
              2015-12-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/UFLDL/" itemprop="url" rel="index">
                    <span itemprop="name">UFLDL</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody">
            
              <h2 id="Sparse_Autoencoder_Recap_3A"><a href="#Sparse_Autoencoder_Recap_3A" class="headerlink" title="Sparse Autoencoder Recap:"></a>Sparse Autoencoder Recap:</h2><p>Because we used a sigmoid activation function for f(z(3)), we needed to constrain or scale the inputs to be in the range [0,1], since the sigmoid function outputs numbers in the range [0,1].(sparse autoencoder输入层mean 0的原因)。</p>
<h2 id="Linear_Decoder_3A"><a href="#Linear_Decoder_3A" class="headerlink" title="Linear Decoder:"></a>Linear Decoder:</h2><p>only in the output layer that we use the linear activation function.<br>输出不再限制于【0，1】.所以不需要再使输入限制在【0，1】之间。原理就是output层的z不再用simoid方程的偏导进行处理，而是直接将z赋值给a，但是其他hidden layer层不变。</p>
<h2 id="u4E60_u9898_u7B54_u6848_uFF1A"><a href="#u4E60_u9898_u7B54_u6848_uFF1A" class="headerlink" title="习题答案："></a>习题答案：</h2><ul>
<li>linearDecoderExercise.m</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% CS294A/CS294W Linear Decoder Exercise</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%  Instructions</span></span><br><span class="line"><span class="comment">%  ------------</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">%  This file contains code that helps you get started on the</span></span><br><span class="line"><span class="comment">%  linear decoder exericse. For this exercise, you will only need to modify</span></span><br><span class="line"><span class="comment">%  the code in sparseAutoencoderLinearCost.m. You will not need to modify</span></span><br><span class="line"><span class="comment">%  any code in this file.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 0: Initialization</span></span><br><span class="line"><span class="comment">%  Here we initialize some parameters used for the exercise.</span></span><br><span class="line"></span><br><span class="line">imageChannels = <span class="number">3</span>;     <span class="comment">% number of channels (rgb, so 3)</span></span><br><span class="line"></span><br><span class="line">patchDim   = <span class="number">8</span>;          <span class="comment">% patch dimension</span></span><br><span class="line">numPatches = <span class="number">100000</span>;   <span class="comment">% number of patches</span></span><br><span class="line"></span><br><span class="line">visibleSize = patchDim * patchDim * imageChannels;  <span class="comment">% number of input units </span></span><br><span class="line">outputSize  = visibleSize;   <span class="comment">% number of output units</span></span><br><span class="line">hiddenSize  = <span class="number">400</span>;           <span class="comment">% number of hidden units </span></span><br><span class="line"></span><br><span class="line">sparsityParam = <span class="number">0.035</span>; <span class="comment">% desired average activation of the hidden units.</span></span><br><span class="line">lambda = <span class="number">3e-3</span>;         <span class="comment">% weight decay parameter       </span></span><br><span class="line"><span class="built_in">beta</span> = <span class="number">5</span>;              <span class="comment">% weight of sparsity penalty term       </span></span><br><span class="line"></span><br><span class="line">epsilon = <span class="number">0.1</span>;	       <span class="comment">% epsilon for ZCA whitening</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 1: Create and modify sparseAutoencoderLinearCost.m to use a linear decoder,</span></span><br><span class="line"><span class="comment">%          and check gradients</span></span><br><span class="line"><span class="comment">%  You should copy sparseAutoencoderCost.m from your earlier exercise </span></span><br><span class="line"><span class="comment">%  and rename it to sparseAutoencoderLinearCost.m. </span></span><br><span class="line"><span class="comment">%  Then you need to rename the function from sparseAutoencoderCost to</span></span><br><span class="line"><span class="comment">%  sparseAutoencoderLinearCost, and modify it so that the sparse autoencoder</span></span><br><span class="line"><span class="comment">%  uses a linear decoder instead. Once that is done, you should check </span></span><br><span class="line"><span class="comment">% your gradients to verify that they are correct.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% <span class="doctag">NOTE:</span> Modify sparseAutoencoderCost first!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% To speed up gradient checking, we will use a reduced network and some</span></span><br><span class="line"><span class="comment">% dummy patches</span></span><br><span class="line"></span><br><span class="line">debugHiddenSize = <span class="number">5</span>;</span><br><span class="line">debugvisibleSize = <span class="number">8</span>;</span><br><span class="line">patches = <span class="built_in">rand</span>(<span class="matrix">[<span class="number">8</span> <span class="number">10</span>]</span>);</span><br><span class="line">theta = initializeParameters(debugHiddenSize, debugvisibleSize); </span><br><span class="line"></span><br><span class="line"><span class="matrix">[cost, grad]</span> = sparseAutoencoderLinearCost(theta, debugvisibleSize, debugHiddenSize, ...</span><br><span class="line">                                           lambda, sparsityParam, <span class="built_in">beta</span>, ...</span><br><span class="line">                                           patches);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Check gradients</span></span><br><span class="line">numGrad = computeNumericalGradient( @(x) sparseAutoencoderLinearCost(x, debugvisibleSize, debugHiddenSize, ...</span><br><span class="line">                                                  lambda, sparsityParam, <span class="built_in">beta</span>, ...</span><br><span class="line">                                                  patches), theta);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Use this to visually compare the gradients side by side</span></span><br><span class="line"><span class="built_in">disp</span>(<span class="matrix">[numGrad grad]</span>); </span><br><span class="line"></span><br><span class="line">diff = norm(numGrad-grad)/norm(numGrad+grad);</span><br><span class="line"><span class="comment">% Should be small. In our implementation, these values are usually less than 1e-9.</span></span><br><span class="line"><span class="built_in">disp</span>(diff); </span><br><span class="line"></span><br><span class="line">assert(diff &lt; <span class="number">1e-9</span>, <span class="string">'Difference too large. Check your gradient computation again'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% <span class="doctag">NOTE:</span> Once your gradients check out, you should run step 0 again to</span></span><br><span class="line"><span class="comment">%       reinitialize the parameters</span></span><br><span class="line"><span class="comment">%&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%======================================================================</span></span><br><span class="line"><span class="comment">%% STEP 2: Learn features on small patches</span></span><br><span class="line"><span class="comment">%  In this step, you will use your sparse autoencoder (which now uses a </span></span><br><span class="line"><span class="comment">%  linear decoder) to learn features on small patches sampled from related</span></span><br><span class="line"><span class="comment">%  images.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% STEP 2a: Load patches</span></span><br><span class="line"><span class="comment">%  In this step, we load 100k patches sampled from the STL10 dataset and</span></span><br><span class="line"><span class="comment">%  visualize them. Note that these patches have been scaled to [0,1]</span></span><br><span class="line"></span><br><span class="line">load stlSampledPatches.mat</span><br><span class="line"></span><br><span class="line">displayColorNetwork(patches(:, <span class="number">1</span>:<span class="number">100</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">%% STEP 2b: Apply preprocessing</span></span><br><span class="line"><span class="comment">%  In this sub-step, we preprocess the sampled patches, in particular, </span></span><br><span class="line"><span class="comment">%  ZCA whitening them. </span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">%  In a later exercise on convolution and pooling, you will need to replicate </span></span><br><span class="line"><span class="comment">%  exactly the preprocessing steps you apply to these patches before </span></span><br><span class="line"><span class="comment">%  using the autoencoder to learn features on them. Hence, we will save the</span></span><br><span class="line"><span class="comment">%  ZCA whitening and mean image matrices together with the learned features</span></span><br><span class="line"><span class="comment">%  later on.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Subtract mean patch (hence zeroing the mean of the patches)</span></span><br><span class="line">meanPatch = mean(patches, <span class="number">2</span>);  </span><br><span class="line">patches = <span class="built_in">bsxfun</span>(@minus, patches, meanPatch);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Apply ZCA whitening</span></span><br><span class="line">sigma = patches * patches<span class="operator">'</span> / numPatches;</span><br><span class="line"><span class="matrix">[u, s, v]</span> = svd(sigma);</span><br><span class="line">ZCAWhite = u * <span class="built_in">diag</span>(<span class="number">1</span> ./ <span class="built_in">sqrt</span>(<span class="built_in">diag</span>(s) + epsilon)) * u<span class="operator">'</span>;</span><br><span class="line">patches = ZCAWhite * patches;</span><br><span class="line"></span><br><span class="line">displayColorNetwork(patches(:, <span class="number">1</span>:<span class="number">100</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">%% STEP 2c: Learn features</span></span><br><span class="line"><span class="comment">%  You will now use your sparse autoencoder (with linear decoder) to learn</span></span><br><span class="line"><span class="comment">%  features on the preprocessed patches. This should take around 45 minutes.</span></span><br><span class="line"></span><br><span class="line">theta = initializeParameters(hiddenSize, visibleSize);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Use minFunc to minimize the function</span></span><br><span class="line">addpath minFunc/</span><br><span class="line"></span><br><span class="line">options = struct;</span><br><span class="line">options.Method = <span class="string">'lbfgs'</span>; </span><br><span class="line">options.maxIter = <span class="number">400</span>;</span><br><span class="line">options.display = <span class="string">'on'</span>;</span><br><span class="line"></span><br><span class="line"><span class="matrix">[optTheta, cost]</span> = minFunc( @(p) sparseAutoencoderLinearCost(p, ...</span><br><span class="line">                                   visibleSize, hiddenSize, ...</span><br><span class="line">                                   lambda, sparsityParam, ...</span><br><span class="line">                                   <span class="built_in">beta</span>, patches), ...</span><br><span class="line">                              theta, options);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Save the learned features and the preprocessing matrices for use in </span></span><br><span class="line"><span class="comment">% the later exercise on convolution and pooling</span></span><br><span class="line">fprintf(<span class="string">'Saving learned features and preprocessing matrices...\n'</span>);                          </span><br><span class="line">save(<span class="string">'STL10Features.mat'</span>, <span class="string">'optTheta'</span>, <span class="string">'ZCAWhite'</span>, <span class="string">'meanPatch'</span>);</span><br><span class="line">fprintf(<span class="string">'Saved\n'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% STEP 2d: Visualize learned features</span></span><br><span class="line"></span><br><span class="line">W = <span class="built_in">reshape</span>(optTheta(<span class="number">1</span>:visibleSize * hiddenSize), hiddenSize, visibleSize);</span><br><span class="line">b = optTheta(<span class="number">2</span>*hiddenSize*visibleSize+<span class="number">1</span>:<span class="number">2</span>*hiddenSize*visibleSize+hiddenSize);</span><br><span class="line">displayColorNetwork( (W*ZCAWhite)<span class="operator">'</span>);</span><br></pre></td></tr></table></figure>
<ul>
<li>sparseAutoencoderLinearCost.m</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">% -------------------- YOUR CODE HERE --------------------</span></span><br><span class="line"><span class="comment">% Instructions:</span></span><br><span class="line"><span class="comment">%   Copy sparseAutoencoderCost in sparseAutoencoderCost.m from your</span></span><br><span class="line"><span class="comment">%   earlier exercise onto this file, renaming the function to</span></span><br><span class="line"><span class="comment">%   sparseAutoencoderLinearCost, and changing the autoencoder to use a</span></span><br><span class="line"><span class="comment">%   linear decoder.</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[cost,grad]</span> = <span class="title">sparseAutoencoderLinearCost</span><span class="params">(theta, visibleSize, hiddenSize, ...</span><br><span class="line">                                             lambda, sparsityParam, beta, data)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">% visibleSize: the number of input units (probably 64) </span></span><br><span class="line"><span class="comment">% hiddenSize: the number of hidden units (probably 25) </span></span><br><span class="line"><span class="comment">% lambda: weight decay parameter</span></span><br><span class="line"><span class="comment">% sparsityParam: The desired average activation for the hidden units (denoted in the lecture</span></span><br><span class="line"><span class="comment">%                           notes by the greek alphabet rho, which looks like a lower-case "p").</span></span><br><span class="line"><span class="comment">% beta: weight of sparsity penalty term</span></span><br><span class="line"><span class="comment">% data: Our 64x10000 matrix containing the training data.  So, data(:,i) is the i-th training example. </span></span><br><span class="line">  </span><br><span class="line"><span class="comment">% The input theta is a vector (because minFunc expects the parameters to be a vector). </span></span><br><span class="line"><span class="comment">% We first convert theta to the (W1, W2, b1, b2) matrix/vector format, so that this </span></span><br><span class="line"><span class="comment">% follows the notation convention of the lecture notes. </span></span><br><span class="line"></span><br><span class="line">W1 = <span class="built_in">reshape</span>(theta(<span class="number">1</span>:hiddenSize*visibleSize), hiddenSize, visibleSize);</span><br><span class="line">W2 = <span class="built_in">reshape</span>(theta(hiddenSize*visibleSize+<span class="number">1</span>:<span class="number">2</span>*hiddenSize*visibleSize), visibleSize, hiddenSize);</span><br><span class="line">b1 = theta(<span class="number">2</span>*hiddenSize*visibleSize+<span class="number">1</span>:<span class="number">2</span>*hiddenSize*visibleSize+hiddenSize);</span><br><span class="line">b2 = theta(<span class="number">2</span>*hiddenSize*visibleSize+hiddenSize+<span class="number">1</span>:<span class="keyword">end</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Cost and gradient variables (your code needs to compute these values). </span></span><br><span class="line"><span class="comment">% Here, we initialize them to zeros. </span></span><br><span class="line">cost = <span class="number">0</span>;</span><br><span class="line">W1grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(W1)); </span><br><span class="line">W2grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(W2));</span><br><span class="line">b1grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(b1)); </span><br><span class="line">b2grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(b2));</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------- YOUR CODE HERE --------------------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Compute the cost/optimization objective J_sparse(W,b) for the Sparse Autoencoder,</span></span><br><span class="line"><span class="comment">%                and the corresponding gradients W1grad, W2grad, b1grad, b2grad.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% W1grad, W2grad, b1grad and b2grad should be computed using backpropagation.</span></span><br><span class="line"><span class="comment">% Note that W1grad has the same dimensions as W1, b1grad has the same dimensions</span></span><br><span class="line"><span class="comment">% as b1, etc.  Your code should set W1grad to be the partial derivative of J_sparse(W,b) with</span></span><br><span class="line"><span class="comment">% respect to W1.  I.e., W1grad(i,j) should be the partial derivative of J_sparse(W,b) </span></span><br><span class="line"><span class="comment">% with respect to the input parameter W1(i,j).  Thus, W1grad should be equal to the term </span></span><br><span class="line"><span class="comment">% [(1/m) \Delta W^&#123;(1)&#125; + \lambda W^&#123;(1)&#125;] in the last block of pseudo-code in Section 2.2 </span></span><br><span class="line"><span class="comment">% of the lecture notes (and similarly for W2grad, b1grad, b2grad).</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% Stated differently, if we were using batch gradient descent to optimize the parameters,</span></span><br><span class="line"><span class="comment">% the gradient descent update to W1 would be W1 := W1 - alpha * W1grad, and similarly for W2, b1, b2. </span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">%H=zeros(size(data));</span></span><br><span class="line"><span class="comment">%m=size(data,2);</span></span><br><span class="line"><span class="comment">%sparsity_vec=zeros(hiddenSize,1);</span></span><br><span class="line"><span class="comment">%for index = 1:(m/1000)   </span></span><br><span class="line">  <span class="comment">% z2=W1*data(:,index)+b1;</span></span><br><span class="line">   <span class="comment">%a2=sigmoid(z2);</span></span><br><span class="line">   </span><br><span class="line">  <span class="comment">% for q = 1:hiddenSize</span></span><br><span class="line">   <span class="comment">%    sparsity_vec(q)=(1/m) *sum(sum( a2(q).*data));</span></span><br><span class="line">   <span class="comment">%end</span></span><br><span class="line"> <span class="comment">%  sparsity_delta=beta* (-(sparsityParam./sparsity_vec) + ( (1-sparsityParam)./(1.-sparsity_vec) ));</span></span><br><span class="line">   </span><br><span class="line">  <span class="comment">% z3=W2*a2+b2;</span></span><br><span class="line">   <span class="comment">%a3=sigmoid(z3);</span></span><br><span class="line">   <span class="comment">%H(:,index)=a3;</span></span><br><span class="line">   <span class="comment">%delta3=-(data(:,index)-a3)   .*   ( a3.*(1-a3) );</span></span><br><span class="line">   <span class="comment">%delta2=(W2'*delta3+sparsity_delta) .* (a2.*(1-a2));</span></span><br><span class="line">   <span class="comment">%if u want to use gradient checking,</span></span><br><span class="line">   <span class="comment">%make sure that</span></span><br><span class="line">   <span class="comment">%delta3*a2'=g(theta)</span></span><br><span class="line">   <span class="comment">%W2grad =W2grad + delta3 * a2';</span></span><br><span class="line">   <span class="comment">%b2grad =b2grad + delta3;</span></span><br><span class="line">   <span class="comment">%W1grad = W1grad + delta2 * data(:,index)';</span></span><br><span class="line">   <span class="comment">%b1grad =b1grad + delta2; </span></span><br><span class="line"><span class="comment">%end</span></span><br><span class="line"><span class="comment">%alpha=10;</span></span><br><span class="line"><span class="comment">%W1=W1 - alpha*  ( ((1/m)*W1grad) +lambda*W1 );</span></span><br><span class="line"><span class="comment">%b1=b1 - alpha*((1/m)*b1grad);</span></span><br><span class="line"><span class="comment">%W2=W2 - alpha*  ( ((1/m)*W2grad) +lambda*W2 );</span></span><br><span class="line"><span class="comment">%b2=b2 - alpha*((1/m)*b2grad);</span></span><br><span class="line"><span class="comment">%J=(1/(2*m)) * sum(sum((H-data).^2)) + (lambda/2) * ( sum(sum(W1.^2)) + sum(sum(W2.^2)) );</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%sparsity1=0;</span></span><br><span class="line"><span class="comment">%for j = 1:hiddenSize</span></span><br><span class="line"> <span class="comment">%   mid=(1/m) *sum(sum( a2(j)*data));</span></span><br><span class="line"> <span class="comment">%   sparsity1=sparsityParam*log(sparsityParam/mid) + (1-sparsityParam) * log((1-sparsityParam)/(1-mid));</span></span><br><span class="line">    <span class="comment">%cost=J + beta* sparsity1;</span></span><br><span class="line"><span class="comment">%end</span></span><br><span class="line">Jcost = <span class="number">0</span>;<span class="comment">%直接误差</span></span><br><span class="line">Jweight = <span class="number">0</span>;<span class="comment">%权值惩罚</span></span><br><span class="line">Jsparse = <span class="number">0</span>;<span class="comment">%稀疏性惩罚</span></span><br><span class="line"><span class="matrix">[n m]</span> = <span class="built_in">size</span>(data);<span class="comment">%m为样本的个数，n为样本的特征数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%前向算法计算各神经网络节点的线性组合值和active值</span></span><br><span class="line">z2 = W1*data+<span class="built_in">repmat</span>(b1,<span class="number">1</span>,m);<span class="comment">%注意这里一定要将b1向量复制扩展成m列的矩阵</span></span><br><span class="line">a2 = sigmoid(z2);</span><br><span class="line">z3 = W2*a2+<span class="built_in">repmat</span>(b2,<span class="number">1</span>,m);</span><br><span class="line">a3 = z3;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算预测产生的误差</span></span><br><span class="line">Jcost = (<span class="number">0.5</span>/m)*sum(sum((a3-data).^<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">%计算权值惩罚项</span></span><br><span class="line">Jweight = (<span class="number">1</span>/<span class="number">2</span>)*(sum(sum(W1.^<span class="number">2</span>))+sum(sum(W2.^<span class="number">2</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">%计算稀释性规则项</span></span><br><span class="line">rho = (<span class="number">1</span>/m).*sum(a2,<span class="number">2</span>);<span class="comment">%求出第一个隐含层的平均值向量</span></span><br><span class="line">Jsparse = sum(sparsityParam.*<span class="built_in">log</span>(sparsityParam./rho)+ ...</span><br><span class="line">        (<span class="number">1</span>-sparsityParam).*<span class="built_in">log</span>((<span class="number">1</span>-sparsityParam)./(<span class="number">1</span>-rho)));</span><br><span class="line"></span><br><span class="line"><span class="comment">%损失函数的总表达式</span></span><br><span class="line">cost = Jcost+lambda*Jweight+<span class="built_in">beta</span>*Jsparse;</span><br><span class="line"></span><br><span class="line"><span class="comment">%反向算法求出每个节点的误差值</span></span><br><span class="line">d3 = -(data-a3);</span><br><span class="line">sterm = <span class="built_in">beta</span>*(-sparsityParam./rho+(<span class="number">1</span>-sparsityParam)./(<span class="number">1</span>-rho));<span class="comment">%因为加入了稀疏规则项，所以</span></span><br><span class="line">                                                             <span class="comment">%计算偏导时需要引入该项</span></span><br><span class="line">d2 = (W2<span class="operator">'</span>*d3+<span class="built_in">repmat</span>(sterm,<span class="number">1</span>,m)).*sigmoidInv(z2); </span><br><span class="line"></span><br><span class="line"><span class="comment">%计算W1grad </span></span><br><span class="line">W1grad = W1grad+d2*data<span class="operator">'</span>;</span><br><span class="line">W1grad = (<span class="number">1</span>/m)*W1grad+lambda*W1;</span><br><span class="line"></span><br><span class="line"><span class="comment">%计算W2grad  </span></span><br><span class="line">W2grad = W2grad+d3*a2<span class="operator">'</span>;</span><br><span class="line">W2grad = (<span class="number">1</span>/m).*W2grad+lambda*W2;</span><br><span class="line"></span><br><span class="line"><span class="comment">%计算b1grad </span></span><br><span class="line">b1grad = b1grad+sum(d2,<span class="number">2</span>);</span><br><span class="line">b1grad = (<span class="number">1</span>/m)*b1grad;<span class="comment">%注意b的偏导是一个向量，所以这里应该把每一行的值累加起来</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%计算b2grad </span></span><br><span class="line">b2grad = b2grad+sum(d3,<span class="number">2</span>);</span><br><span class="line">b2grad = (<span class="number">1</span>/m)*b2grad;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%-------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">% After computing the cost and gradient, we will convert the gradients back</span></span><br><span class="line"><span class="comment">% to a vector format (suitable for minFunc).  Specifically, we will unroll</span></span><br><span class="line"><span class="comment">% your gradient matrices into a vector.</span></span><br><span class="line"></span><br><span class="line">grad = <span class="matrix">[W1grad(:) ; W2grad(:) ; b1grad(:) ; b2grad(:)]</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%-------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">% Here's an implementation of the sigmoid function, which you may find useful</span></span><br><span class="line"><span class="comment">% in your computation of the costs and the gradients.  This inputs a (row or</span></span><br><span class="line"><span class="comment">% column) vector (say (z1, z2, z3)) and returns (f(z1), f(z2), f(z3)). </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sigm</span> = <span class="title">sigmoid</span><span class="params">(x)</span></span></span><br><span class="line">  </span><br><span class="line">    sigm = <span class="number">1</span> ./ (<span class="number">1</span> + <span class="built_in">exp</span>(-x));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sigmInv</span> = <span class="title">sigmoidInv</span><span class="params">(x)</span></span></span><br><span class="line">    sigmInv = sigmoid(x).*(<span class="number">1</span>-sigmoid(x));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------- YOUR CODE HERE --------------------</span></span><br></pre></td></tr></table></figure>

            
          </span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2015/12/08/UFLDL/DeepLearningbyAndrewNg---PCAandwhitening/" itemprop="url">
                  UFLDL-PCA
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2015-12-08T13:08:00+08:00" content="2015-12-08">
              2015-12-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/UFLDL/" itemprop="url" rel="index">
                    <span itemprop="name">UFLDL</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody">
            
              <p>这是UFLDL的编程练习。具体教程参照官网。</p>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p><img src="http://i.imgur.com/WfzL6Ph.png" alt=""></p>
<h2 id="Whitening"><a href="#Whitening" class="headerlink" title="Whitening"></a>Whitening</h2><p>白化是一种重要的预处理过程，其目的就是降低输入数据的冗余性，使得经过白化处理的输入数据具有如下性质：(i)特征之间相关性较低；(ii)所有特征具有相同的方差。</p>
<p>白化处理分PCA白化和ZCA白化，PCA白化保证数据各维度的方差为1，而ZCA白化保证数据各维度的方差相同。PCA白化可以用于降维也可以去相关性，而ZCA白化主要用于去相关性，且尽量使白化后的数据接近原始输入数据。</p>
<p>PCA白化ZCA白化都降低了特征之间相关性较低，同时使得所有特征具有相同的方差。</p>
<ol>
<li>PCA白化需要保证数据各维度的方差为1，ZCA白化只需保证方差相等。</li>
<li>PCA白化可进行降维也可以去相关性，而ZCA白化主要用于去相关性另外。</li>
<li>ZCA白化相比于PCA白化使得处理后的数据更加的接近原始数据。<h2 id="Regularizaion"><a href="#Regularizaion" class="headerlink" title="Regularizaion"></a>Regularizaion</h2><img src="http://i.imgur.com/ELFjG7X.png" alt=""><h2 id="u7F16_u7A0B_u4F5C_u4E1A_u4EE3_u7801_uFF08_u5EFA_u8BAE_u4F5C_u4E3A_u53C2_u8003_uFF0C_u81EA_u5DF1_u5148_u72EC_u7ACB_u5B8C_u6210_uFF09_uFF1A"><a href="#u7F16_u7A0B_u4F5C_u4E1A_u4EE3_u7801_uFF08_u5EFA_u8BAE_u4F5C_u4E3A_u53C2_u8003_uFF0C_u81EA_u5DF1_u5148_u72EC_u7ACB_u5B8C_u6210_uFF09_uFF1A" class="headerlink" title="编程作业代码（建议作为参考，自己先独立完成）："></a>编程作业代码（建议作为参考，自己先独立完成）：</h2></li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% Step 0a: Load data</span></span><br><span class="line"><span class="comment">%  Here we provide the code to load natural image data into x.</span></span><br><span class="line"><span class="comment">%  x will be a 144 * 10000 matrix, where the kth column x(:, k) corresponds to</span></span><br><span class="line"><span class="comment">%  the raw image data from the kth 12x12 image patch sampled.</span></span><br><span class="line"><span class="comment">%  You do not need to change the code below.</span></span><br><span class="line"></span><br><span class="line">x = sampleIMAGESRAW();</span><br><span class="line">figure(<span class="string">'name'</span>,<span class="string">'Raw images'</span>);</span><br><span class="line">randsel = randi(<span class="built_in">size</span>(x,<span class="number">2</span>),<span class="number">200</span>,<span class="number">1</span>); <span class="comment">% A random selection of samples for visualization</span></span><br><span class="line">display_network(x(:,randsel));</span><br><span class="line"></span><br><span class="line"><span class="comment">%%================================================================</span></span><br><span class="line"><span class="comment">%% Step 0b: Zero-mean the data (by row)</span></span><br><span class="line"><span class="comment">%  You can make use of the mean and repmat/bsxfun functions.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------- YOUR CODE HERE -------------------- </span></span><br><span class="line">avg = mean(x,<span class="number">1</span>);</span><br><span class="line">x = x - <span class="built_in">repmat</span>(avg,<span class="built_in">size</span>(x,<span class="number">1</span>),<span class="number">1</span>);</span><br><span class="line"><span class="comment">%%================================================================</span></span><br><span class="line"><span class="comment">%% Step 1a: Implement PCA to obtain xRot</span></span><br><span class="line"><span class="comment">%  Implement PCA to obtain xRot, the matrix in which the data is expressed</span></span><br><span class="line"><span class="comment">%  with respect to the eigenbasis of sigma, which is the matrix U.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------- YOUR CODE HERE -------------------- </span></span><br><span class="line">xRot = <span class="built_in">zeros</span>(<span class="built_in">size</span>(x)); <span class="comment">% You need to compute this</span></span><br><span class="line">U = <span class="built_in">zeros</span>(<span class="built_in">size</span>(x,<span class="number">1</span>));</span><br><span class="line">sigma = x*x<span class="operator">'</span>/<span class="built_in">size</span>(x,<span class="number">2</span>);</span><br><span class="line"><span class="matrix">[U,S,V]</span> = svd(sigma);</span><br><span class="line">xRot = U<span class="operator">'</span>*x;</span><br><span class="line"></span><br><span class="line"><span class="comment">%%================================================================</span></span><br><span class="line"><span class="comment">%% Step 1b: Check your implementation of PCA</span></span><br><span class="line"><span class="comment">%  The covariance matrix for the data expressed with respect to the basis U</span></span><br><span class="line"><span class="comment">%  should be a diagonal matrix with non-zero entries only along the main</span></span><br><span class="line"><span class="comment">%  diagonal. We will verify this here.</span></span><br><span class="line"><span class="comment">%  Write code to compute the covariance matrix, covar. </span></span><br><span class="line"><span class="comment">%  When visualised as an image, you should see a straight line across the</span></span><br><span class="line"><span class="comment">%  diagonal (non-zero entries) against a blue background (zero entries).</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------- YOUR CODE HERE -------------------- </span></span><br><span class="line">covar = <span class="built_in">zeros</span>(<span class="built_in">size</span>(x, <span class="number">1</span>)); <span class="comment">% You need to compute this</span></span><br><span class="line"><span class="comment">%[covar,S,V] = svd(sigma);</span></span><br><span class="line">covar = xRot*xRot<span class="operator">'</span>/<span class="built_in">size</span>(xRot,<span class="number">2</span>) ;</span><br><span class="line"><span class="comment">% Visualise the covariance matrix. You should see a line across the</span></span><br><span class="line"><span class="comment">% diagonal against a blue background.</span></span><br><span class="line">figure(<span class="string">'name'</span>,<span class="string">'Visualisation of covariance matrix'</span>);</span><br><span class="line">imagesc(covar);</span><br><span class="line"></span><br><span class="line"><span class="comment">%%================================================================</span></span><br><span class="line"><span class="comment">%% Step 2: Find k, the number of components to retain</span></span><br><span class="line"><span class="comment">%  Write code to determine k, the number of components to retain in order</span></span><br><span class="line"><span class="comment">%  to retain at least 99% of the variance.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------- YOUR CODE HERE -------------------- </span></span><br><span class="line">k = <span class="number">0</span>; <span class="comment">% Set k accordingly</span></span><br><span class="line">POVV = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span>:<span class="built_in">size</span>(x,<span class="number">1</span>)</span><br><span class="line">  POVV = sum(sum(S(<span class="number">1</span>:k,<span class="number">1</span>:k)))/sum(sum(S));</span><br><span class="line">    <span class="keyword">if</span> POVV &gt;=<span class="number">0.99</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    k    </span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%================================================================</span></span><br><span class="line"><span class="comment">%% Step 3: Implement PCA with dimension reduction</span></span><br><span class="line"><span class="comment">%  Now that you have found k, you can reduce the dimension of the data by</span></span><br><span class="line"><span class="comment">%  discarding the remaining dimensions. In this way, you can represent the</span></span><br><span class="line"><span class="comment">%  data in k dimensions instead of the original 144, which will save you</span></span><br><span class="line"><span class="comment">%  computational time when running learning algorithms on the reduced</span></span><br><span class="line"><span class="comment">%  representation.</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">%  Following the dimension reduction, invert the PCA transformation to produce </span></span><br><span class="line"><span class="comment">%  the matrix xHat, the dimension-reduced data with respect to the original basis.</span></span><br><span class="line"><span class="comment">%  Visualise the data and compare it to the raw data. You will observe that</span></span><br><span class="line"><span class="comment">%  there is little loss due to throwing away the principal components that</span></span><br><span class="line"><span class="comment">%  correspond to dimensions with low variation.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------- YOUR CODE HERE -------------------- </span></span><br><span class="line">xHat = <span class="built_in">zeros</span>(<span class="built_in">size</span>(x));  <span class="comment">% You need to compute this</span></span><br><span class="line">xRot = U(:,<span class="number">1</span>:k)<span class="operator">'</span>*x;</span><br><span class="line">xHat = U(:,<span class="number">1</span>:k)*xRot;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Visualise the data, and compare it to the raw data</span></span><br><span class="line"><span class="comment">% You should observe that the raw and processed data are of comparable quality.</span></span><br><span class="line"><span class="comment">% For comparison, you may wish to generate a PCA reduced image which</span></span><br><span class="line"><span class="comment">% retains only 90% of the variance.</span></span><br><span class="line"></span><br><span class="line">figure(<span class="string">'name'</span>,<span class="matrix">[<span class="string">'PCA processed images '</span>,sprintf(<span class="string">'(%d / %d dimensions)'</span>, k, size(x, <span class="number">1</span>)),<span class="string">''</span>]</span>);</span><br><span class="line">display_network(xHat(:,randsel));</span><br><span class="line">figure(<span class="string">'name'</span>,<span class="string">'Raw images'</span>);</span><br><span class="line">display_network(x(:,randsel));</span><br><span class="line"></span><br><span class="line"><span class="comment">%%================================================================</span></span><br><span class="line"><span class="comment">%% Step 4a: Implement PCA with whitening and regularisation</span></span><br><span class="line"><span class="comment">%  Implement PCA with whitening and regularisation to produce the matrix</span></span><br><span class="line"><span class="comment">%  xPCAWhite. </span></span><br><span class="line"></span><br><span class="line">epsilon = <span class="number">0.1</span>;</span><br><span class="line">xPCAWhite = <span class="built_in">zeros</span>(<span class="built_in">size</span>(x));</span><br><span class="line">xPCAWhite = <span class="built_in">diag</span>(<span class="number">1.</span>/<span class="built_in">sqrt</span>(<span class="built_in">diag</span>(S)+epsilon))*U<span class="operator">'</span>*x;</span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------- YOUR CODE HERE -------------------- </span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%================================================================</span></span><br><span class="line"><span class="comment">%% Step 4b: Check your implementation of PCA whitening </span></span><br><span class="line"><span class="comment">%  Check your implementation of PCA whitening with and without regularisation. </span></span><br><span class="line"><span class="comment">%  PCA whitening without regularisation results a covariance matrix </span></span><br><span class="line"><span class="comment">%  that is equal to the identity matrix. PCA whitening with regularisation</span></span><br><span class="line"><span class="comment">%  results in a covariance matrix with diagonal entries starting close to </span></span><br><span class="line"><span class="comment">%  1 and gradually becoming smaller. We will verify these properties here.</span></span><br><span class="line"><span class="comment">%  Write code to compute the covariance matrix, covar. </span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%  Without regularisation (set epsilon to 0 or close to 0), </span></span><br><span class="line"><span class="comment">%  when visualised as an image, you should see a red line across the</span></span><br><span class="line"><span class="comment">%  diagonal (one entries) against a blue background (zero entries).</span></span><br><span class="line"><span class="comment">%  With regularisation, you should see a red line that slowly turns</span></span><br><span class="line"><span class="comment">%  blue across the diagonal, corresponding to the one entries slowly</span></span><br><span class="line"><span class="comment">%  becoming smaller.</span></span><br><span class="line"><span class="comment">% -------------------- YOUR CODE HERE -------------------- </span></span><br><span class="line">covar = xPCAWhite*xPCAWhite<span class="operator">'</span>/<span class="built_in">size</span>(xPCAWhite,<span class="number">2</span>);</span><br><span class="line"><span class="comment">% Visualise the covariance matrix. You should see a red line across the</span></span><br><span class="line"><span class="comment">% diagonal against a blue background.</span></span><br><span class="line">figure(<span class="string">'name'</span>,<span class="string">'Visualisation of covariance matrix'</span>);</span><br><span class="line">imagesc(covar);</span><br><span class="line"></span><br><span class="line"><span class="comment">%%================================================================</span></span><br><span class="line"><span class="comment">%% Step 5: Implement ZCA whitening</span></span><br><span class="line"><span class="comment">%  Now implement ZCA whitening to produce the matrix xZCAWhite. </span></span><br><span class="line"><span class="comment">%  Visualise the data and compare it to the raw data. You should observe</span></span><br><span class="line"><span class="comment">%  that whitening results in, among other things, enhanced edges.</span></span><br><span class="line">epsilon = <span class="number">0.1</span>;</span><br><span class="line">xZCAWhite = <span class="built_in">zeros</span>(<span class="built_in">size</span>(x));</span><br><span class="line">xZCAWhite = U*<span class="built_in">diag</span>(<span class="number">1.</span>/<span class="built_in">sqrt</span>(<span class="built_in">diag</span>(S)+epsilon))*U<span class="operator">'</span>*x;</span><br><span class="line"></span><br><span class="line"><span class="comment">% -------------------- YOUR CODE HERE -------------------- </span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Visualise the data, and compare it to the raw data.</span></span><br><span class="line"><span class="comment">% You should observe that the whitened images have enhanced edges.</span></span><br><span class="line">figure(<span class="string">'name'</span>,<span class="string">'ZCA whitened images'</span>);</span><br><span class="line">display_network(xZCAWhite(:,randsel));</span><br><span class="line">figure(<span class="string">'name'</span>,<span class="string">'Raw images'</span>);</span><br><span class="line">display_network(x(:,randsel));</span><br></pre></td></tr></table></figure>
            
          </span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    

  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



        </div>

        


        

      </div>

      
        
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/default_avatar.jpg" alt="John Doe" itemprop="image"/>
          <p class="site-author-name" itemprop="name">John Doe</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">31</span>
              <span class="site-state-item-name">文章</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            
              <span class="site-state-item-count">5</span>
              <span class="site-state-item-name">分類</span>
              
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">0</span>
              <span class="site-state-item-name">標籤</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


      
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2015</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 強力驅動
</div>

<div class="theme-info">
  主題 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>



      </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  

  
    
    

  


  

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
<script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

<script type="text/javascript" src="/js/motion.js?v=0.4.5.2" id="motion.global"></script>


  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  

  <script type="text/javascript" src="/js/bootstrap.js"></script>

  
  

  
  

</body>
</html>
